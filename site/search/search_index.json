{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Tinkerbell Docs Everything you need to know about Tinkerbell and its major component microservices. \u200b What is Tinkerbell? Tinkerbell is an open-source, bare metal provisioning engine, built and maintained by the team at Packet. Interested in contributing? Check out our Contributors' Page . What's Powering Tinkerbell? The Tinkerbell stack consists of five microservices, and a grpc API: Tink - Tink is the Tinkerbell server and CLI. It communicates over gRPC, and is responsible for processing workflows. The CLI is used to create workflows and their building blocks, templates and hardware data. Boots - Boots is Tinkerbell's DHCP server. It handles DHCP requests, hands out IPs, and serves up iPXE . It uses the Tinkerbell client to pull and push hardware data. It only responds to a predefined set of MAC addresses so it can be deployed in an existing network without interfering with existing DHCP infrastructure. Hegel - Hegel is the metadata service used by Tinkerbell and OSIE. It collects data from both and transforms it into a JSON format to be consumed as metadata. OSIE - OSIE is an in-memory installation environment for bare metal. It installs operating systems and handles deprovisioning. PBnJ - PBnJ is a microservice that can communicate with baseboard management controllers (BMCs) to control power and boot settings. In addition to the microservices, there are three pieces of infrastructure: PostgreSQL - Tinkerbell uses PostgreSQL as its data store. PostgreSQL is a free and open-source relational database management system, and it stores Tinkerbell's hardware data, templates, and workflows. Image Repository - Tinkerbell uses a local image repository to store all of the action images used in a workflow. This is particularly useful for secure environments that don't have access to the internet. You can also choose to use Quay or DockerHub as the repository to store images for if your environment does have internet access. NGINX - NGINX is a web server which can also be used as a reverse proxy, load balancer, mail proxy, and HTTP cache. Tinkerbell uses NGINX to serve the required boot files and OS images during workflow execution. First Steps \u200bNew to Tinkerbell or bare metal provisioning? This is a great place to start! Getting Started - Set up Tinkerbell locally with vagrant or on Packet with Terraform . Run hello world to see Tinkerbell in action.\u200b Get Help Need a little help getting started? We're here! Check out the FAQs - When there are questions, we document the answers. Join our Community Slack . Look for the #tinkerbell channel. Submit an issue on Github .","title":"Home"},{"location":"#the-tinkerbell-docs","text":"Everything you need to know about Tinkerbell and its major component microservices. \u200b","title":"The Tinkerbell Docs"},{"location":"#what-is-tinkerbell","text":"Tinkerbell is an open-source, bare metal provisioning engine, built and maintained by the team at Packet. Interested in contributing? Check out our Contributors' Page .","title":"What is Tinkerbell?"},{"location":"#whats-powering-tinkerbell","text":"The Tinkerbell stack consists of five microservices, and a grpc API: Tink - Tink is the Tinkerbell server and CLI. It communicates over gRPC, and is responsible for processing workflows. The CLI is used to create workflows and their building blocks, templates and hardware data. Boots - Boots is Tinkerbell's DHCP server. It handles DHCP requests, hands out IPs, and serves up iPXE . It uses the Tinkerbell client to pull and push hardware data. It only responds to a predefined set of MAC addresses so it can be deployed in an existing network without interfering with existing DHCP infrastructure. Hegel - Hegel is the metadata service used by Tinkerbell and OSIE. It collects data from both and transforms it into a JSON format to be consumed as metadata. OSIE - OSIE is an in-memory installation environment for bare metal. It installs operating systems and handles deprovisioning. PBnJ - PBnJ is a microservice that can communicate with baseboard management controllers (BMCs) to control power and boot settings. In addition to the microservices, there are three pieces of infrastructure: PostgreSQL - Tinkerbell uses PostgreSQL as its data store. PostgreSQL is a free and open-source relational database management system, and it stores Tinkerbell's hardware data, templates, and workflows. Image Repository - Tinkerbell uses a local image repository to store all of the action images used in a workflow. This is particularly useful for secure environments that don't have access to the internet. You can also choose to use Quay or DockerHub as the repository to store images for if your environment does have internet access. NGINX - NGINX is a web server which can also be used as a reverse proxy, load balancer, mail proxy, and HTTP cache. Tinkerbell uses NGINX to serve the required boot files and OS images during workflow execution.","title":"What's Powering Tinkerbell?"},{"location":"#first-steps","text":"\u200bNew to Tinkerbell or bare metal provisioning? This is a great place to start! Getting Started - Set up Tinkerbell locally with vagrant or on Packet with Terraform . Run hello world to see Tinkerbell in action.\u200b","title":"First Steps"},{"location":"#get-help","text":"Need a little help getting started? We're here! Check out the FAQs - When there are questions, we document the answers. Join our Community Slack . Look for the #tinkerbell channel. Submit an issue on Github .","title":"Get Help"},{"location":"troubleshooting/","text":"Troubleshooting The following are possible errors you might run into and their possible solutions: If you are on Packet environment and the worker is not set to always PXE, then you may encounter the following error. The issue can be resolved by setting the worker to always PXE from the server actions dropdown. PowerEdge R6415 - BIOS 1.8.7 A system restart is required. The system detected an exception during the UEFI pre-boot environment. On a provisioner, the registry login can fail with the error below. This is generally because the /certs/ca.pem file (generated by certs container) is not copied to /etc/docker/certs.d/<registry-host>/ca.crt . If the error occurs on a worker, then verify if the certificate file is present at /var/tinkerbell/nginx/workflow/ca.pem location on the provisioner. Login did not succeed, error: Error response from daemon: Get https://<registry-host>/v2/: x509: certificate signed by unknown authority (possibly because of \"crypto/rsa: verification error\" while trying to verify candidate authority certificate \"Autogenerated CA\")","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"The following are possible errors you might run into and their possible solutions: If you are on Packet environment and the worker is not set to always PXE, then you may encounter the following error. The issue can be resolved by setting the worker to always PXE from the server actions dropdown. PowerEdge R6415 - BIOS 1.8.7 A system restart is required. The system detected an exception during the UEFI pre-boot environment. On a provisioner, the registry login can fail with the error below. This is generally because the /certs/ca.pem file (generated by certs container) is not copied to /etc/docker/certs.d/<registry-host>/ca.crt . If the error occurs on a worker, then verify if the certificate file is present at /var/tinkerbell/nginx/workflow/ca.pem location on the provisioner. Login did not succeed, error: Error response from daemon: Get https://<registry-host>/v2/: x509: certificate signed by unknown authority (possibly because of \"crypto/rsa: verification error\" while trying to verify candidate authority certificate \"Autogenerated CA\")","title":"Troubleshooting"},{"location":"about/architecture/","text":"Architecture Provisioner The provisioner machine is the main driver for executing a workflow. The Provisioner houses and runs the Tinkerbell stack , acts as the DHCP server, keeps track of hardware data, templates, and workflows. You may divide these components into multiple servers that would then all function as your Provisioner. Provisioner Requirements OS The Tinkerbell stack has been tested on Ubuntu 16.04 and CentOS 7. Minimum Resources CPU - 2vCPUs RAM - 4 GB Disk - 20 GB, this includes the OS Network L2 networking is required for the ability to run a DHCP server (in this case, Boots). Worker A Worker machine is the target machine, identified by its hardware data, that calls back to the Provisioner and executes its specified workflows. Any machine that has had its hardware data pushed into Tinkerbell can become a part of a workflow. A Worker can be a part of multiple workflows. Worker Requirements There are some very basic requirements that a Worker machine must meet in order to be able to boot and call back to the Provisioner. They must be able to boot from network using iPXE. 4 GB of RAM for OSIE boot and operation. There are no Disk requirements for a Worker since OSIE runs an in-memory operating system. Your disk requirements will be determined by the OS you are going to install and other use-case considerations.","title":"Architecture"},{"location":"about/architecture/#architecture","text":"","title":"Architecture"},{"location":"about/architecture/#provisioner","text":"The provisioner machine is the main driver for executing a workflow. The Provisioner houses and runs the Tinkerbell stack , acts as the DHCP server, keeps track of hardware data, templates, and workflows. You may divide these components into multiple servers that would then all function as your Provisioner.","title":"Provisioner"},{"location":"about/architecture/#provisioner-requirements","text":"OS The Tinkerbell stack has been tested on Ubuntu 16.04 and CentOS 7. Minimum Resources CPU - 2vCPUs RAM - 4 GB Disk - 20 GB, this includes the OS Network L2 networking is required for the ability to run a DHCP server (in this case, Boots).","title":"Provisioner Requirements"},{"location":"about/architecture/#worker","text":"A Worker machine is the target machine, identified by its hardware data, that calls back to the Provisioner and executes its specified workflows. Any machine that has had its hardware data pushed into Tinkerbell can become a part of a workflow. A Worker can be a part of multiple workflows.","title":"Worker"},{"location":"about/architecture/#worker-requirements","text":"There are some very basic requirements that a Worker machine must meet in order to be able to boot and call back to the Provisioner. They must be able to boot from network using iPXE. 4 GB of RAM for OSIE boot and operation. There are no Disk requirements for a Worker since OSIE runs an in-memory operating system. Your disk requirements will be determined by the OS you are going to install and other use-case considerations.","title":"Worker Requirements"},{"location":"about/hardware-data/","text":"Hardware Data Hardware data holds the details about the hardware that you wish to use with a workflow. A hardware may have multiple network devices that can be used in a worklfow. The details about all those devices is maintained in JSON format as hardware data. Example If you have a hardware that has a single network/worker device on it, its hardware data shall be structured like the following: { \"id\": \"0eba0bf8-3772-4b4a-ab9f-6ebe93b90a94\", \"metadata\": { \"bonding_mode\": 5, \"custom\": { \"preinstalled_operating_system_version\": {}, \"private_subnets\": [] }, \"facility\": { \"facility_code\": \"ewr1\", \"plan_slug\": \"c2.medium.x86\", \"plan_version_slug\": \"\" }, \"instance\": { \"crypted_root_password\": \"redacted\", \"operating_system_version\": { \"distro\": \"ubuntu\", \"os_slug\": \"ubuntu_18_04\", \"version\": \"18.04\" }, \"storage\": { \"disks\": [ { \"device\": \"/dev/sda\", \"partitions\": [ { \"label\": \"BIOS\", \"number\": 1, \"size\": 4096 }, { \"label\": \"SWAP\", \"number\": 2, \"size\": 3993600 }, { \"label\": \"ROOT\", \"number\": 3, \"size\": 0 } ], \"wipe_table\": true } ], \"filesystems\": [ { \"mount\": { \"create\": { \"options\": [\"-L\", \"ROOT\"] }, \"device\": \"/dev/sda3\", \"format\": \"ext4\", \"point\": \"/\" } }, { \"mount\": { \"create\": { \"options\": [\"-L\", \"SWAP\"] }, \"device\": \"/dev/sda2\", \"format\": \"swap\", \"point\": \"none\" } } ] } }, \"manufacturer\": { \"id\": \"\", \"slug\": \"\" }, \"state\": \"\" }, \"network\": { \"interfaces\": [ { \"dhcp\": { \"arch\": \"x86_64\", \"hostname\": \"server001\", \"ip\": { \"address\": \"192.168.1.5\", \"gateway\": \"192.168.1.1\", \"netmask\": \"255.255.255.248\" }, \"lease_time\": 86400, \"mac\": \"00:00:00:00:00:00\", \"name_servers\": [], \"time_servers\": [], \"uefi\": false }, \"netboot\": { \"allow_pxe\": true, \"allow_workflow\": true, \"ipxe\": { \"contents\": \"#!ipxe\", \"url\": \"http://url/menu.ipxe\" }, \"osie\": { \"base_url\": \"\", \"initrd\": \"\", \"kernel\": \"vmlinuz-x86_64\" } } } ] } } Property Description The following section explains each property in the above example: Property Description id A UUID used to uniquely identify the hardware. The id can be generated using the uuidgen command. If you are in Packet environment, you can get the id from the server overview page. network Network details network.Interfaces[] List of network interfaces on the hardware network.interfaces[].dhcp DHCP details network.interfaces[].dhcp.mac MAC address of the network device (worker) network.interfaces[].dhcp.ip IP details for DHCP network.interfaces[].dhcp.ip.address Worker IP address to be requested over DHCP network.interfaces[].dhcp.ip.gateway Gateway address network.interfaces[].dhcp.ip.netmask Netmask for the private network network.interfaces[].dhcp.hostname Hostname network.interfaces[].dhcp.lease_time Expiration in secs (default: 86400) network.interfaces[].dhcp.name_servers[] DNS servers network.interfaces[].dhcp.time_servers[] NTP servers network.interfaces[].dhcp.arch Hardware architecture, example: x86_64 network.interfaces[].dhcp.uefi Is UEFI network.interfaces[].netboot Netboot details network.interfaces[].netboot.allow_pxe Must be set to true to PXE. network.interfaces[].netboot.allow_workflow Must be true in order to execute a workflow. network.interfaces[].netboot.ipxe Details for iPXE network.interfaces[].netboot.ipxe.url iPXE script URL network.interfaces[].netboot.ipxe.contents iPXE script contents network.interfaces[].netboot.osie OSIE details network.interfaces[].netboot.osie.kernel Kernel network.interfaces[].netboot.osie.initrd Initrd network.interfaces[].netboot.osie.base_url Base URL for the kernel and initrd metadata Hardware metadata details metadata.state State must be set to provisioning for workflows. metadata.bonding_mode Bonding mode metadata.manufacturer Manufacturer details metadata.instance Holds the details for an instance metadata.instance.storage Details for an instance storage like disks and filesystems metadata.instance.storage.disks List of disk partitions metadata.instance.storage.disks[].device Name of the disk metadata.instance.storage.disks[].wipe_table Set to true to allow disk wipe. metadata.instance.storage.disks[].partitions List of disk partitions metadata.instance.storage.disks[].partitions[].size Size of the partition metadata.instance.storage.disks[].partitions[].label Partition label like BIOS, SWAP or ROOT metadata.instance.storage.disks[].partitions[].number Partition number metadata.instance.storage.filesystems List of filesystems and their respective mount points metadata.instance.storage.filesystems[].mount Details about the filesystem to be mounted metadata.instance.storage.filesystems[].mount.point Mount point for the filesystem metadata.instance.storage.filesystems[].mount.create Additional details that can be provided while creating a partition metadata.instance.storage.filesystems[].mount.create.options Options to be passed to mkfs while creating a partition metadata.instance.storage.filesystems[].mount.device Device to be mounted metadata.instance.storage.filesystems[].mount.format Filesystem format metadata.instance.crypted_root_password Hash for root password that is used to login into the worker after provisioning. The hash can be generated using the openssl passwd command. For example, openssl passwd -6 -salt xyz your-password . metadata.operating_system_version Details about the operating system to be installed metadata.operating_system_version.distro Operating system distribution name, like ubuntu metadata.operating_system_version.version Operating system version, like 18.04 or 20.04 metadata.operating_system_version.os_slug A slug is a combination of operating system distro and version. metadata.facility Facility details metadata.facility.plan_slug The slug for the worker class. The value for this property depends on how you setup your workflow. While it is required if you are using the OS images from packet-images repository, it may be left out if not used at all in the workflow. metadata.facility.facility_code For local setup, onprem or any other string value can be used. The Minimal Hardware Data While the hardware data is essential, not all the properties are required for every workflow. In fact, it's upto a workflow designer how they want to use the data in their workflow. Therefore, you may start with the minimal data given below and only add the properties you would want to use in your workflow. { \"id\": \"0eba0bf8-3772-4b4a-ab9f-6ebe93b90a94\", \"metadata\": { \"facility\": { \"facility_code\": \"ewr1\", \"plan_slug\": \"c2.medium.x86\", \"plan_version_slug\": \"\" }, \"instance\": {}, \"state\": \"provisioning\" }, \"network\": { \"interfaces\": [ { \"dhcp\": { \"arch\": \"x86_64\", \"ip\": { \"address\": \"192.168.1.5\", \"gateway\": \"192.168.1.1\", \"netmask\": \"255.255.255.248\" }, \"mac\": \"00:00:00:00:00:00\", \"uefi\": false }, \"netboot\": { \"allow_pxe\": true, \"allow_workflow\": true } } ] } }","title":"Hardware Data"},{"location":"about/hardware-data/#hardware-data","text":"Hardware data holds the details about the hardware that you wish to use with a workflow. A hardware may have multiple network devices that can be used in a worklfow. The details about all those devices is maintained in JSON format as hardware data.","title":"Hardware Data"},{"location":"about/hardware-data/#example","text":"If you have a hardware that has a single network/worker device on it, its hardware data shall be structured like the following: { \"id\": \"0eba0bf8-3772-4b4a-ab9f-6ebe93b90a94\", \"metadata\": { \"bonding_mode\": 5, \"custom\": { \"preinstalled_operating_system_version\": {}, \"private_subnets\": [] }, \"facility\": { \"facility_code\": \"ewr1\", \"plan_slug\": \"c2.medium.x86\", \"plan_version_slug\": \"\" }, \"instance\": { \"crypted_root_password\": \"redacted\", \"operating_system_version\": { \"distro\": \"ubuntu\", \"os_slug\": \"ubuntu_18_04\", \"version\": \"18.04\" }, \"storage\": { \"disks\": [ { \"device\": \"/dev/sda\", \"partitions\": [ { \"label\": \"BIOS\", \"number\": 1, \"size\": 4096 }, { \"label\": \"SWAP\", \"number\": 2, \"size\": 3993600 }, { \"label\": \"ROOT\", \"number\": 3, \"size\": 0 } ], \"wipe_table\": true } ], \"filesystems\": [ { \"mount\": { \"create\": { \"options\": [\"-L\", \"ROOT\"] }, \"device\": \"/dev/sda3\", \"format\": \"ext4\", \"point\": \"/\" } }, { \"mount\": { \"create\": { \"options\": [\"-L\", \"SWAP\"] }, \"device\": \"/dev/sda2\", \"format\": \"swap\", \"point\": \"none\" } } ] } }, \"manufacturer\": { \"id\": \"\", \"slug\": \"\" }, \"state\": \"\" }, \"network\": { \"interfaces\": [ { \"dhcp\": { \"arch\": \"x86_64\", \"hostname\": \"server001\", \"ip\": { \"address\": \"192.168.1.5\", \"gateway\": \"192.168.1.1\", \"netmask\": \"255.255.255.248\" }, \"lease_time\": 86400, \"mac\": \"00:00:00:00:00:00\", \"name_servers\": [], \"time_servers\": [], \"uefi\": false }, \"netboot\": { \"allow_pxe\": true, \"allow_workflow\": true, \"ipxe\": { \"contents\": \"#!ipxe\", \"url\": \"http://url/menu.ipxe\" }, \"osie\": { \"base_url\": \"\", \"initrd\": \"\", \"kernel\": \"vmlinuz-x86_64\" } } } ] } }","title":"Example"},{"location":"about/hardware-data/#property-description","text":"The following section explains each property in the above example: Property Description id A UUID used to uniquely identify the hardware. The id can be generated using the uuidgen command. If you are in Packet environment, you can get the id from the server overview page. network Network details network.Interfaces[] List of network interfaces on the hardware network.interfaces[].dhcp DHCP details network.interfaces[].dhcp.mac MAC address of the network device (worker) network.interfaces[].dhcp.ip IP details for DHCP network.interfaces[].dhcp.ip.address Worker IP address to be requested over DHCP network.interfaces[].dhcp.ip.gateway Gateway address network.interfaces[].dhcp.ip.netmask Netmask for the private network network.interfaces[].dhcp.hostname Hostname network.interfaces[].dhcp.lease_time Expiration in secs (default: 86400) network.interfaces[].dhcp.name_servers[] DNS servers network.interfaces[].dhcp.time_servers[] NTP servers network.interfaces[].dhcp.arch Hardware architecture, example: x86_64 network.interfaces[].dhcp.uefi Is UEFI network.interfaces[].netboot Netboot details network.interfaces[].netboot.allow_pxe Must be set to true to PXE. network.interfaces[].netboot.allow_workflow Must be true in order to execute a workflow. network.interfaces[].netboot.ipxe Details for iPXE network.interfaces[].netboot.ipxe.url iPXE script URL network.interfaces[].netboot.ipxe.contents iPXE script contents network.interfaces[].netboot.osie OSIE details network.interfaces[].netboot.osie.kernel Kernel network.interfaces[].netboot.osie.initrd Initrd network.interfaces[].netboot.osie.base_url Base URL for the kernel and initrd metadata Hardware metadata details metadata.state State must be set to provisioning for workflows. metadata.bonding_mode Bonding mode metadata.manufacturer Manufacturer details metadata.instance Holds the details for an instance metadata.instance.storage Details for an instance storage like disks and filesystems metadata.instance.storage.disks List of disk partitions metadata.instance.storage.disks[].device Name of the disk metadata.instance.storage.disks[].wipe_table Set to true to allow disk wipe. metadata.instance.storage.disks[].partitions List of disk partitions metadata.instance.storage.disks[].partitions[].size Size of the partition metadata.instance.storage.disks[].partitions[].label Partition label like BIOS, SWAP or ROOT metadata.instance.storage.disks[].partitions[].number Partition number metadata.instance.storage.filesystems List of filesystems and their respective mount points metadata.instance.storage.filesystems[].mount Details about the filesystem to be mounted metadata.instance.storage.filesystems[].mount.point Mount point for the filesystem metadata.instance.storage.filesystems[].mount.create Additional details that can be provided while creating a partition metadata.instance.storage.filesystems[].mount.create.options Options to be passed to mkfs while creating a partition metadata.instance.storage.filesystems[].mount.device Device to be mounted metadata.instance.storage.filesystems[].mount.format Filesystem format metadata.instance.crypted_root_password Hash for root password that is used to login into the worker after provisioning. The hash can be generated using the openssl passwd command. For example, openssl passwd -6 -salt xyz your-password . metadata.operating_system_version Details about the operating system to be installed metadata.operating_system_version.distro Operating system distribution name, like ubuntu metadata.operating_system_version.version Operating system version, like 18.04 or 20.04 metadata.operating_system_version.os_slug A slug is a combination of operating system distro and version. metadata.facility Facility details metadata.facility.plan_slug The slug for the worker class. The value for this property depends on how you setup your workflow. While it is required if you are using the OS images from packet-images repository, it may be left out if not used at all in the workflow. metadata.facility.facility_code For local setup, onprem or any other string value can be used.","title":"Property Description"},{"location":"about/hardware-data/#the-minimal-hardware-data","text":"While the hardware data is essential, not all the properties are required for every workflow. In fact, it's upto a workflow designer how they want to use the data in their workflow. Therefore, you may start with the minimal data given below and only add the properties you would want to use in your workflow. { \"id\": \"0eba0bf8-3772-4b4a-ab9f-6ebe93b90a94\", \"metadata\": { \"facility\": { \"facility_code\": \"ewr1\", \"plan_slug\": \"c2.medium.x86\", \"plan_version_slug\": \"\" }, \"instance\": {}, \"state\": \"provisioning\" }, \"network\": { \"interfaces\": [ { \"dhcp\": { \"arch\": \"x86_64\", \"ip\": { \"address\": \"192.168.1.5\", \"gateway\": \"192.168.1.1\", \"netmask\": \"255.255.255.248\" }, \"mac\": \"00:00:00:00:00:00\", \"uefi\": false }, \"netboot\": { \"allow_pxe\": true, \"allow_workflow\": true } } ] } }","title":"The Minimal Hardware Data"},{"location":"about/templates/","text":"Templates A Template is a YAML file that defines the source of a Workflow by declaring one or more tasks . The tasks are executed sequentially, in the order in which they are declared. Each task consists of one or more actions . Each action contains an image to be executed as part of a workflow, identified by the image field. You can create any script, app, or other set of instructions to be an action image by containerizing it and pushing it into either the local Docker registry included in the Tinkerbell stack or an external image repository. Here is a sample template: version: \"0.1\" name: ubuntu_provisioning global_timeout: 6000 tasks: - name: \"os-installation\" worker: \"{{.device_1}}\" volumes: - /dev:/dev - /dev/console:/dev/console - /lib/firmware:/lib/firmware:ro environment: MIRROR_HOST: <MIRROR_HOST_IP> actions: - name: \"disk-wipe\" image: disk-wipe timeout: 90 - name: \"disk-partition\" image: disk-partition timeout: 600 environment: MIRROR_HOST: <MIRROR_HOST_IP> volumes: - /statedir:/statedir - name: \"install-root-fs\" image: install-root-fs timeout: 600 - name: \"install-grub\" image: install-grub timeout: 600 volumes: - /statedir:/statedir The volumes field contains the volume mappings between the host machine and the docker container where your images are running. The environment field is used to pass environment variables to the images. Each action can have its own volumes and environment variables. Any entry at an action will overwrite the value defined at the task level. For example, in the above template the MIRROR_HOST environment variable defined at action disk-partition will overwrite the value defined at task level. The other actions will receive the original value defined at the task level. The timeout defines the amount of time to wait for an action to execute and is in seconds. A hardware device, such as a Worker's MAC address, is specified in template as keys. {{.device_1}} {{.device_2}} Keys can only contain letters , numbers and underscores . These keys are evaluated during workflow creation, being passed in as an JSON argument to tink workflow create . Templates are each stored as blobs in the database; they are later parsed during the creation of a workflow. Template CLI Commands A Template is pushed to the database on the Provisioner with the tink template create command which returns a generated UUID for the template. A template can be retrieved from the database with that ID and tink template get . Similarly it can be deleted with tink template delete . You can list all the templates that are stored in the database with tink template list . Update the name or the contents of a template with the tink template update command. A complete list of CLI commands and examples is in the CLI Reference .","title":"Templates"},{"location":"about/templates/#templates","text":"A Template is a YAML file that defines the source of a Workflow by declaring one or more tasks . The tasks are executed sequentially, in the order in which they are declared. Each task consists of one or more actions . Each action contains an image to be executed as part of a workflow, identified by the image field. You can create any script, app, or other set of instructions to be an action image by containerizing it and pushing it into either the local Docker registry included in the Tinkerbell stack or an external image repository. Here is a sample template: version: \"0.1\" name: ubuntu_provisioning global_timeout: 6000 tasks: - name: \"os-installation\" worker: \"{{.device_1}}\" volumes: - /dev:/dev - /dev/console:/dev/console - /lib/firmware:/lib/firmware:ro environment: MIRROR_HOST: <MIRROR_HOST_IP> actions: - name: \"disk-wipe\" image: disk-wipe timeout: 90 - name: \"disk-partition\" image: disk-partition timeout: 600 environment: MIRROR_HOST: <MIRROR_HOST_IP> volumes: - /statedir:/statedir - name: \"install-root-fs\" image: install-root-fs timeout: 600 - name: \"install-grub\" image: install-grub timeout: 600 volumes: - /statedir:/statedir The volumes field contains the volume mappings between the host machine and the docker container where your images are running. The environment field is used to pass environment variables to the images. Each action can have its own volumes and environment variables. Any entry at an action will overwrite the value defined at the task level. For example, in the above template the MIRROR_HOST environment variable defined at action disk-partition will overwrite the value defined at task level. The other actions will receive the original value defined at the task level. The timeout defines the amount of time to wait for an action to execute and is in seconds. A hardware device, such as a Worker's MAC address, is specified in template as keys. {{.device_1}} {{.device_2}} Keys can only contain letters , numbers and underscores . These keys are evaluated during workflow creation, being passed in as an JSON argument to tink workflow create . Templates are each stored as blobs in the database; they are later parsed during the creation of a workflow.","title":"Templates"},{"location":"about/templates/#template-cli-commands","text":"A Template is pushed to the database on the Provisioner with the tink template create command which returns a generated UUID for the template. A template can be retrieved from the database with that ID and tink template get . Similarly it can be deleted with tink template delete . You can list all the templates that are stored in the database with tink template list . Update the name or the contents of a template with the tink template update command. A complete list of CLI commands and examples is in the CLI Reference .","title":"Template CLI Commands"},{"location":"about/workflows/","text":"Workflows A workflow is the complete set of operations to be run on a Worker. It consists of two building blocks: a Worker's hardware data and a template . Creating a Workflow You create a workflow with the tink workflow create command, which takes a template ID and a JSON object that identifies the Worker, and combines them into a workflow. The workflow is stored in the database on the Provisioner and returns a workflow ID. For example, tink workflow create \\ -t 75ab8483-6f42-42a9-a80d-a9f6196130df \\ -r '{\"device_1\":\"08:00:27:00:00:01\"}' > Created Workflow: a8984b09-566d-47ba-b6c5-fbe482d8ad7f The template ID is 75ab8483-6f42-42a9-a80d-a9f6196130df . The MAC address of the Worker is 08:00:27:00:00:01 , which should match the MAC address of hardware data that you have already created to identify that Worker. It is mapped to device_1 , which is where the MAC address will be substituted into the template when the workflow is created. After creating a workflow, you can retrieve it from the database by ID with tink workflow get . This is particularly useful to check to see that the MAC address or IP Address of the Worker was correctly substituted when you created the workflow. tink workflow get a8984b09-566d-47ba-b6c5-fbe482d8ad7f > version: \"0.1\" name: hello_world_workflow global_timeout: 600 tasks: - name: \"hello world\" worker: \"08:00:27:00:00:01\" actions: - name: \"hello_world\" image: hello-world timeout: 60 In addition, you can list all the workflows stored in the database with tink workflow list . Delete a workflow with tink workflow delete . Workflow Execution On the first boot, the Worker is PXE booted, asks Boots for it's IP address, and loads into OSIE. It then asks the tink-server for workflows that match its MAC or IP address. Those workflows are then executed onto the Worker. If there are no workflows defined for the Worker, the Provisioner will ignore the Worker's request. If as a part of the workflow, a new OS is installed and completes successfully, then the boot request (after reboot) will be handled by newly installed OS. If as a part of the workflow, an OS is not installed, then the Worker after reboot will request PXE-boot from the Provisioner. You can view the events and the state of a workflow during or after its execution with the tink CLI using the tink workflow events an the tink workflow state commands. Ephemeral Data Ephemeral data is data that is shared between Workers as they execute workflows. Ephemeral data is stored at /workflow/<workflow_id> in each tink-worker. Initially the directory is empty; you populate with it by having a template's actions (scripts, etc) write to it. Then, the content in /workflow/<workflow_id> is pushed back to the database and from the database, pushed out to the other Workers. As the workflow progresses, subsequent actions on a Worker can read any ephemeral data that's been created by previous actions on other Workers, as well as update that file with any changes. Ephemeral data is only preserved through the life of a single workflow. Each workflow that executes gets an empty file. The data can take the form of a light JSON like below, or some binary files that other workers might require to complete their action. There is a 10 MB limit for ephemeral data, because it gets pushed to and from the tink-server and tink-worker with every action, so it needs to be pretty light. For instance, a Worker may write the following data: { \"instance_id\": \"123e4567-e89b-12d3-a456-426655440000\", \"mac_addr\": \"F5:C9:E2:99:BD:9B\", \"operating_system\": \"ubuntu_18_04\" } The other worker may retrieve and use this data and eventually add some more: { \"instance_id\": \"123e4567-e89b-12d3-a456-426655440000\", \"ip_addresses\": [ { \"address_family\": 4, \"address\": \"172.27.0.23\", \"cidr\": 31, \"private\": true } ], \"mac_addr\": \"F5:C9:E2:99:BD:9B\", \"operating_system\": \"ubuntu_18_04\" } You can get the ephemeral data associated with a workflow with the tink workflow data tink CLI command.","title":"Workflows"},{"location":"about/workflows/#workflows","text":"A workflow is the complete set of operations to be run on a Worker. It consists of two building blocks: a Worker's hardware data and a template .","title":"Workflows"},{"location":"about/workflows/#creating-a-workflow","text":"You create a workflow with the tink workflow create command, which takes a template ID and a JSON object that identifies the Worker, and combines them into a workflow. The workflow is stored in the database on the Provisioner and returns a workflow ID. For example, tink workflow create \\ -t 75ab8483-6f42-42a9-a80d-a9f6196130df \\ -r '{\"device_1\":\"08:00:27:00:00:01\"}' > Created Workflow: a8984b09-566d-47ba-b6c5-fbe482d8ad7f The template ID is 75ab8483-6f42-42a9-a80d-a9f6196130df . The MAC address of the Worker is 08:00:27:00:00:01 , which should match the MAC address of hardware data that you have already created to identify that Worker. It is mapped to device_1 , which is where the MAC address will be substituted into the template when the workflow is created. After creating a workflow, you can retrieve it from the database by ID with tink workflow get . This is particularly useful to check to see that the MAC address or IP Address of the Worker was correctly substituted when you created the workflow. tink workflow get a8984b09-566d-47ba-b6c5-fbe482d8ad7f > version: \"0.1\" name: hello_world_workflow global_timeout: 600 tasks: - name: \"hello world\" worker: \"08:00:27:00:00:01\" actions: - name: \"hello_world\" image: hello-world timeout: 60 In addition, you can list all the workflows stored in the database with tink workflow list . Delete a workflow with tink workflow delete .","title":"Creating a Workflow"},{"location":"about/workflows/#workflow-execution","text":"On the first boot, the Worker is PXE booted, asks Boots for it's IP address, and loads into OSIE. It then asks the tink-server for workflows that match its MAC or IP address. Those workflows are then executed onto the Worker. If there are no workflows defined for the Worker, the Provisioner will ignore the Worker's request. If as a part of the workflow, a new OS is installed and completes successfully, then the boot request (after reboot) will be handled by newly installed OS. If as a part of the workflow, an OS is not installed, then the Worker after reboot will request PXE-boot from the Provisioner. You can view the events and the state of a workflow during or after its execution with the tink CLI using the tink workflow events an the tink workflow state commands.","title":"Workflow Execution"},{"location":"about/workflows/#ephemeral-data","text":"Ephemeral data is data that is shared between Workers as they execute workflows. Ephemeral data is stored at /workflow/<workflow_id> in each tink-worker. Initially the directory is empty; you populate with it by having a template's actions (scripts, etc) write to it. Then, the content in /workflow/<workflow_id> is pushed back to the database and from the database, pushed out to the other Workers. As the workflow progresses, subsequent actions on a Worker can read any ephemeral data that's been created by previous actions on other Workers, as well as update that file with any changes. Ephemeral data is only preserved through the life of a single workflow. Each workflow that executes gets an empty file. The data can take the form of a light JSON like below, or some binary files that other workers might require to complete their action. There is a 10 MB limit for ephemeral data, because it gets pushed to and from the tink-server and tink-worker with every action, so it needs to be pretty light. For instance, a Worker may write the following data: { \"instance_id\": \"123e4567-e89b-12d3-a456-426655440000\", \"mac_addr\": \"F5:C9:E2:99:BD:9B\", \"operating_system\": \"ubuntu_18_04\" } The other worker may retrieve and use this data and eventually add some more: { \"instance_id\": \"123e4567-e89b-12d3-a456-426655440000\", \"ip_addresses\": [ { \"address_family\": 4, \"address\": \"172.27.0.23\", \"cidr\": 31, \"private\": true } ], \"mac_addr\": \"F5:C9:E2:99:BD:9B\", \"operating_system\": \"ubuntu_18_04\" } You can get the ephemeral data associated with a workflow with the tink workflow data tink CLI command.","title":"Ephemeral Data"},{"location":"cli-reference/hardware/","text":"Hardware Operations delete Delete hardware data by ID. id Get hardware data by ID. ip Get hardware data by an associated IP Address. list List the hardware data in the database. mac Get hardware data by an associated MAC Address. push Push new hardware data to the database. watch Watch hardware data for changes. tink hardware --help - Displays the available commands and usage for tink hardware . tink hardware delete Deletes the specified hardware data. tink hardware delete <ID> [--help] [--facility] Arguments ID - The ID of the hardware data you want to delete from the database. Use multiple IDs to delete more than one hardware data at a time. Options -h , --help - Displays usage for delete . -f , --facility - string used to build grpc and http urls Examples tink hardware delete 0eba0bf8-3772-4b4a-ab9f-6ebe93b90a25 > 2020/07/01 15:01:04 Hardware data with id 0eba0bf8-3772-4b4a-ab9f-6ebe93b90a25 deleted successfully tink hardware id Returns hardware data for the specified ID or IDs as JSON objects. tink hardware id <ID> [--help] [--facility] Arguments ID - The ID of the hardware data you want to retrieve from the database. Use multiple IDs to retrieve more than one hardware data at a time. Options -h , --help - Displays usage for id . -f , --facility - string used to build grpc and http urls Examples tink hardware id 0eba0bf8-3772-4b4a-ab9f-6ebe93b90a94 > {\"metadata\":{\"instance\":{},\"facility\":{\"facility_code\":\"onprem\"}},\"network\":{\"interfaces\":[{\"dhcp\":{\"mac\":\"08:00:27:00:00:01\",\"arch\":\"x86_64\",\"ip\":{\"address\":\"192.168.1.5\",\"netmask\":\"255.255.255.248\",\"gateway\":\"192.168.1.1\"}},\"netboot\":{\"allow_pxe\":true,\"allow_workflow\":true}}]},\"id\":\"0eba0bf8-3772-4b4a-ab9f-6ebe93b90a94\"} tink hardware ip Returns hardware data for the specified IP Address or IP Addresses as JSON objects. tink hardware id <IP> [--details] [--help] [--facility] Arguments IP - The IP address of the hardware data you want to retrieve from the database. Use multiple IP addresses to retrieve more than one hardware data at a time. Options -d , --details - Displays the entire hardware data JSON for the specified IP address. -h , --help - Displays usage for ip . -f , --facility - string used to build grpc and http urls Examples tink hardware ip --details 192.168.1.5 > {\"metadata\":{\"instance\":{},\"facility\":{\"facility_code\":\"onprem\"}},\"network\":{\"interfaces\":[{\"dhcp\":{\"mac\":\"08:00:27:00:00:01\",\"arch\":\"x86_64\",\"ip\":{\"address\":\"192.168.1.5\",\"netmask\":\"255.255.255.248\",\"gateway\":\"192.168.1.1\"}},\"netboot\":{\"allow_pxe\":true,\"allow_workflow\":true}}]},\"id\":\"0eba0bf8-3772-4b4a-ab9f-6ebe93b90a94\"} tink hardware list Returns a list of all the hardware data that is currently stored in the database. tink hardware list Options -h , --help - Displays usage for list . -f , --facility - string used to build grpc and http urls Examples tink hardware list > +--------------------------------------+-------------------+-------------+-----------+ | ID | MAC ADDRESS | IP ADDRESS | HOSTNAME | +--------------------------------------+-------------------+-------------+-----------+ | f9f56dff-098a-4c5f-a51c-19ad35de85d2 | 98:03:9b:89:d7:da | 192.168.1.4 | localhost | | f9f56dff-098a-4c5f-a51c-19ad35de85d1 | 98:03:9b:89:d7:ba | 192.168.1.5 | worker_1 | +--------------------------------------+-------------------+-------------+-----------+ tink hardware mac Returns hardware data for the specified MAC Address or MAC Addresses as JSON objects. tink hardware mac <MAC> [--details] [--help] [--facility] Arguments MAC - The MAC address of the hardware data you want to retrieve from the database. Use multiple MAC addresses to retrieve more than one hardware data at a time. Options -d , --details - Displays the entire hardware data JSON for the specified MAC address. -h , --help - Displays usage for mac . -f , --facility - string used to build grpc and http urls Examples tink hardware mac --details 08:00:27:00:00:01 > {\"metadata\":{\"instance\":{},\"facility\":{\"facility_code\":\"onprem\"}},\"network\":{\"interfaces\":[{\"dhcp\":{\"mac\":\"08:00:27:00:00:01\",\"arch\":\"x86_64\",\"ip\":{\"address\":\"192.168.1.5\",\"netmask\":\"255.255.255.248\",\"gateway\":\"192.168.1.1\"}},\"netboot\":{\"allow_pxe\":true,\"allow_workflow\":true}}]},\"id\":\"0eba0bf8-3772-4b4a-ab9f-6ebe93b90a94\" tink hardware push Pushes the JSON-formatted hardware data from the specified file into the database. tink hardware push --file <JSON_FILE> [--help] [--facility] Or cat <JSON_FILE> | tink hardware push Or tink hardware push < ./<JSON_FILE> Arguments JSON_FILE - The file where the hardware data is defined in JSON format. Options --file , < ./<JSON_FILE> , cat <JSON_FILE> - Specify the file containing the hardware data JSON. Alternatively, you can open and read the file instead. -h , --help - Displays usage for push . -f , --facility - string used to build grpc and http urls Examples tink hardware push --file data.json > 2020/07/01 20:11:24 Hardware data pushed successfully cat data.json | tink hardware push > 2020/07/01 20:14:18 Hardware data pushed successfully tink hardware push < ./data.json > 2020/07/01 20:11:24 Hardware data pushed successfully tink hardware watch Watch the specified hardware data for changes. tink hardware watch <ID or IDs> [--help] [--facility] Arguments ID - The ID of the hardware data you want to monitor for changes. Use multiple IDs to watch more than one hardware data. Options -h , --help - Displays usage for watch . -f , --facility - string used to build grpc and http urls Examples tink hardware watch 0eba0bf8-3772-4b4a-ab9f-6ebe93b90a94 >","title":"Hardware"},{"location":"cli-reference/hardware/#hardware-operations","text":"delete Delete hardware data by ID. id Get hardware data by ID. ip Get hardware data by an associated IP Address. list List the hardware data in the database. mac Get hardware data by an associated MAC Address. push Push new hardware data to the database. watch Watch hardware data for changes. tink hardware --help - Displays the available commands and usage for tink hardware .","title":"Hardware Operations"},{"location":"cli-reference/hardware/#tink-hardware-delete","text":"Deletes the specified hardware data. tink hardware delete <ID> [--help] [--facility] Arguments ID - The ID of the hardware data you want to delete from the database. Use multiple IDs to delete more than one hardware data at a time. Options -h , --help - Displays usage for delete . -f , --facility - string used to build grpc and http urls Examples tink hardware delete 0eba0bf8-3772-4b4a-ab9f-6ebe93b90a25 > 2020/07/01 15:01:04 Hardware data with id 0eba0bf8-3772-4b4a-ab9f-6ebe93b90a25 deleted successfully","title":"tink hardware delete"},{"location":"cli-reference/hardware/#tink-hardware-id","text":"Returns hardware data for the specified ID or IDs as JSON objects. tink hardware id <ID> [--help] [--facility] Arguments ID - The ID of the hardware data you want to retrieve from the database. Use multiple IDs to retrieve more than one hardware data at a time. Options -h , --help - Displays usage for id . -f , --facility - string used to build grpc and http urls Examples tink hardware id 0eba0bf8-3772-4b4a-ab9f-6ebe93b90a94 > {\"metadata\":{\"instance\":{},\"facility\":{\"facility_code\":\"onprem\"}},\"network\":{\"interfaces\":[{\"dhcp\":{\"mac\":\"08:00:27:00:00:01\",\"arch\":\"x86_64\",\"ip\":{\"address\":\"192.168.1.5\",\"netmask\":\"255.255.255.248\",\"gateway\":\"192.168.1.1\"}},\"netboot\":{\"allow_pxe\":true,\"allow_workflow\":true}}]},\"id\":\"0eba0bf8-3772-4b4a-ab9f-6ebe93b90a94\"}","title":"tink hardware id"},{"location":"cli-reference/hardware/#tink-hardware-ip","text":"Returns hardware data for the specified IP Address or IP Addresses as JSON objects. tink hardware id <IP> [--details] [--help] [--facility] Arguments IP - The IP address of the hardware data you want to retrieve from the database. Use multiple IP addresses to retrieve more than one hardware data at a time. Options -d , --details - Displays the entire hardware data JSON for the specified IP address. -h , --help - Displays usage for ip . -f , --facility - string used to build grpc and http urls Examples tink hardware ip --details 192.168.1.5 > {\"metadata\":{\"instance\":{},\"facility\":{\"facility_code\":\"onprem\"}},\"network\":{\"interfaces\":[{\"dhcp\":{\"mac\":\"08:00:27:00:00:01\",\"arch\":\"x86_64\",\"ip\":{\"address\":\"192.168.1.5\",\"netmask\":\"255.255.255.248\",\"gateway\":\"192.168.1.1\"}},\"netboot\":{\"allow_pxe\":true,\"allow_workflow\":true}}]},\"id\":\"0eba0bf8-3772-4b4a-ab9f-6ebe93b90a94\"}","title":"tink hardware ip"},{"location":"cli-reference/hardware/#tink-hardware-list","text":"Returns a list of all the hardware data that is currently stored in the database. tink hardware list Options -h , --help - Displays usage for list . -f , --facility - string used to build grpc and http urls Examples tink hardware list > +--------------------------------------+-------------------+-------------+-----------+ | ID | MAC ADDRESS | IP ADDRESS | HOSTNAME | +--------------------------------------+-------------------+-------------+-----------+ | f9f56dff-098a-4c5f-a51c-19ad35de85d2 | 98:03:9b:89:d7:da | 192.168.1.4 | localhost | | f9f56dff-098a-4c5f-a51c-19ad35de85d1 | 98:03:9b:89:d7:ba | 192.168.1.5 | worker_1 | +--------------------------------------+-------------------+-------------+-----------+","title":"tink hardware list"},{"location":"cli-reference/hardware/#tink-hardware-mac","text":"Returns hardware data for the specified MAC Address or MAC Addresses as JSON objects. tink hardware mac <MAC> [--details] [--help] [--facility] Arguments MAC - The MAC address of the hardware data you want to retrieve from the database. Use multiple MAC addresses to retrieve more than one hardware data at a time. Options -d , --details - Displays the entire hardware data JSON for the specified MAC address. -h , --help - Displays usage for mac . -f , --facility - string used to build grpc and http urls Examples tink hardware mac --details 08:00:27:00:00:01 > {\"metadata\":{\"instance\":{},\"facility\":{\"facility_code\":\"onprem\"}},\"network\":{\"interfaces\":[{\"dhcp\":{\"mac\":\"08:00:27:00:00:01\",\"arch\":\"x86_64\",\"ip\":{\"address\":\"192.168.1.5\",\"netmask\":\"255.255.255.248\",\"gateway\":\"192.168.1.1\"}},\"netboot\":{\"allow_pxe\":true,\"allow_workflow\":true}}]},\"id\":\"0eba0bf8-3772-4b4a-ab9f-6ebe93b90a94\"","title":"tink hardware mac"},{"location":"cli-reference/hardware/#tink-hardware-push","text":"Pushes the JSON-formatted hardware data from the specified file into the database. tink hardware push --file <JSON_FILE> [--help] [--facility] Or cat <JSON_FILE> | tink hardware push Or tink hardware push < ./<JSON_FILE> Arguments JSON_FILE - The file where the hardware data is defined in JSON format. Options --file , < ./<JSON_FILE> , cat <JSON_FILE> - Specify the file containing the hardware data JSON. Alternatively, you can open and read the file instead. -h , --help - Displays usage for push . -f , --facility - string used to build grpc and http urls Examples tink hardware push --file data.json > 2020/07/01 20:11:24 Hardware data pushed successfully cat data.json | tink hardware push > 2020/07/01 20:14:18 Hardware data pushed successfully tink hardware push < ./data.json > 2020/07/01 20:11:24 Hardware data pushed successfully","title":"tink hardware push"},{"location":"cli-reference/hardware/#tink-hardware-watch","text":"Watch the specified hardware data for changes. tink hardware watch <ID or IDs> [--help] [--facility] Arguments ID - The ID of the hardware data you want to monitor for changes. Use multiple IDs to watch more than one hardware data. Options -h , --help - Displays usage for watch . -f , --facility - string used to build grpc and http urls Examples tink hardware watch 0eba0bf8-3772-4b4a-ab9f-6ebe93b90a94 >","title":"tink hardware watch"},{"location":"cli-reference/template/","text":"Template Operations create Create a template in the database. delete Delete a template from the database. get Get a template by ID. list List all templates in the database. update Update an existing template. tink template --help - Displays the available commands and usage for tink template . tink template create Creates the template from the YAML file, and pushes it to the database. It returns a UUID for the newly created template. tink template create --name <NAME> --path <PATH> [--help] [--facility] Arguments NAME - The name for the new template. PATH - The path to the template file. Options -h , --help - Displays usage information for create . -n , --name - Specify a name for the template. Must be unique and alphanumeric. -p , --path , < ./<PATH> - Path to the template file. Alternatively, you can open and read the file instead. -f , --facility - string used to build grpc and http urls Examples template create --name hello-world < ./hello-world.yml > Created Template: b8dbcf07-39dd-4018-903e-1748ecbd1986 tink template delete Deletes a template from the database. Doesn't return anything. tink template delete <ID> [--help] [--facility] Arguments ID - The ID of the template that you want to delete. Use multiple IDs to delete more than one template at a time. Options -h , --help - Displays usage information for delete . -f , --facility - string used to build grpc and http urls Examples tink template delete b8dbcf07-39dd-4018-903e-1748ecbd1986 > tink template get Returns the specified template or templates in YAML format. tink template get <ID> [--help] [--facility] Arguments ID - The ID of the template you want to retrieve from the database. Use multiple IDs to retrieve more than one template at a time. Options -h , --help - Displays usage information for get . -f , --facility - string used to build grpc and http urls Examples tink template get 160d2cbf-d1ed-496d-9ade-7347b2853cbf > version: \"0.1\" name: hello_world_workflow global_timeout: 600 tasks: - name: \"hello world\" worker: \"{{.device_1}}\" actions: - name: \"hello_world\" image: hello-world timeout: 60 tink template list Lists templates stored in the database in a formatted table. tink template list Options -h , --help - Displays usage information for list . -f , --facility - string used to build grpc and http urls Examples tink template list > +--------------------------------------+-------------------+-------------------------------+-------------------------------+ | TEMPLATE ID | TEMPLATE NAME | CREATED AT | UPDATED AT | +--------------------------------------+-------------------+-------------------------------+-------------------------------+ | 9c7d2a12-8dcb-406c-82a8-f41d2efd8ebf | hello-world-again | 2020-07-06 14:39:19 +0000 UTC | 2020-07-06 14:39:19 +0000 UTC | | 160d2cbf-d1ed-496d-9ade-7347b2853cbf | hello-world | 2020-07-06 14:36:15 +0000 UTC | 2020-07-06 14:36:15 +0000 UTC | +--------------------------------------+-------------------+-------------------------------+-------------------------------+ tink template update Updates an existing template with either a new name, or by specifying a new or updated YAML file. tink template update <ID> [--name <NAME>] [--path <PATH>] [--help] [--facility] Arguments ID - The ID of the template you want to update. Options -h , --help - Displays usage information for update . -n , --name - Specify a new name for the template. Must be unique and alphanumeric. -p , --path , < ./<PATH> - Path to the updated template file. Alternatively, you can open and read the file instead. -f , --facility - string used to build grpc and http urls Examples Update the name of an existing template. tink template update 160d2cbf-d1ed-496d-9ade-7347b2853cbf --name renamed-hello-world > Updated Template: 160d2cbf-d1ed-496d-9ade-7347b2853cbf Update an existing template and keep the same name. tink template update 9c7d2a12-8dcb-406c-82a8-f41d2efd8ebf < ./tmp/new-sample-template.tmpl > Updated Template: 160d2cbf-d1ed-496d-9ade-7347b2853cbf","title":"Template"},{"location":"cli-reference/template/#template-operations","text":"create Create a template in the database. delete Delete a template from the database. get Get a template by ID. list List all templates in the database. update Update an existing template. tink template --help - Displays the available commands and usage for tink template .","title":"Template Operations"},{"location":"cli-reference/template/#tink-template-create","text":"Creates the template from the YAML file, and pushes it to the database. It returns a UUID for the newly created template. tink template create --name <NAME> --path <PATH> [--help] [--facility] Arguments NAME - The name for the new template. PATH - The path to the template file. Options -h , --help - Displays usage information for create . -n , --name - Specify a name for the template. Must be unique and alphanumeric. -p , --path , < ./<PATH> - Path to the template file. Alternatively, you can open and read the file instead. -f , --facility - string used to build grpc and http urls Examples template create --name hello-world < ./hello-world.yml > Created Template: b8dbcf07-39dd-4018-903e-1748ecbd1986","title":"tink template create"},{"location":"cli-reference/template/#tink-template-delete","text":"Deletes a template from the database. Doesn't return anything. tink template delete <ID> [--help] [--facility] Arguments ID - The ID of the template that you want to delete. Use multiple IDs to delete more than one template at a time. Options -h , --help - Displays usage information for delete . -f , --facility - string used to build grpc and http urls Examples tink template delete b8dbcf07-39dd-4018-903e-1748ecbd1986 >","title":"tink template delete"},{"location":"cli-reference/template/#tink-template-get","text":"Returns the specified template or templates in YAML format. tink template get <ID> [--help] [--facility] Arguments ID - The ID of the template you want to retrieve from the database. Use multiple IDs to retrieve more than one template at a time. Options -h , --help - Displays usage information for get . -f , --facility - string used to build grpc and http urls Examples tink template get 160d2cbf-d1ed-496d-9ade-7347b2853cbf > version: \"0.1\" name: hello_world_workflow global_timeout: 600 tasks: - name: \"hello world\" worker: \"{{.device_1}}\" actions: - name: \"hello_world\" image: hello-world timeout: 60","title":"tink template get"},{"location":"cli-reference/template/#tink-template-list","text":"Lists templates stored in the database in a formatted table. tink template list Options -h , --help - Displays usage information for list . -f , --facility - string used to build grpc and http urls Examples tink template list > +--------------------------------------+-------------------+-------------------------------+-------------------------------+ | TEMPLATE ID | TEMPLATE NAME | CREATED AT | UPDATED AT | +--------------------------------------+-------------------+-------------------------------+-------------------------------+ | 9c7d2a12-8dcb-406c-82a8-f41d2efd8ebf | hello-world-again | 2020-07-06 14:39:19 +0000 UTC | 2020-07-06 14:39:19 +0000 UTC | | 160d2cbf-d1ed-496d-9ade-7347b2853cbf | hello-world | 2020-07-06 14:36:15 +0000 UTC | 2020-07-06 14:36:15 +0000 UTC | +--------------------------------------+-------------------+-------------------------------+-------------------------------+","title":"tink template list"},{"location":"cli-reference/template/#tink-template-update","text":"Updates an existing template with either a new name, or by specifying a new or updated YAML file. tink template update <ID> [--name <NAME>] [--path <PATH>] [--help] [--facility] Arguments ID - The ID of the template you want to update. Options -h , --help - Displays usage information for update . -n , --name - Specify a new name for the template. Must be unique and alphanumeric. -p , --path , < ./<PATH> - Path to the updated template file. Alternatively, you can open and read the file instead. -f , --facility - string used to build grpc and http urls Examples Update the name of an existing template. tink template update 160d2cbf-d1ed-496d-9ade-7347b2853cbf --name renamed-hello-world > Updated Template: 160d2cbf-d1ed-496d-9ade-7347b2853cbf Update an existing template and keep the same name. tink template update 9c7d2a12-8dcb-406c-82a8-f41d2efd8ebf < ./tmp/new-sample-template.tmpl > Updated Template: 160d2cbf-d1ed-496d-9ade-7347b2853cbf","title":"tink template update"},{"location":"cli-reference/workflow/","text":"Workflow Operations create Create a workflow. data Get the ephemeral data associated with a workflow. delete Delete a workflow. events Show all events for a workflow. get Get a workflow as it exists in the database. list List all the workflows in the database. state get the current workflow context tink workflow --help - Displays the available commands and usage for tink workflow . tink workflow create Creates a workflow from a template ID and hardware data specified by MAC address or IP address. The workflow is stored in the database and the command returns a workflow ID. tink workflow create --hardware <MAC_ADDRESS or IP_ADDRESS> --template <TEMPLATE_ID> [--help] [--facility] Arguments MAC_ADDRESS or IP_ADDRESS - A JSON object containing the map of MAC or IP addresses of the hardware data used to identify the Worker (or workers) to their variables in the template. The key should match worker field in the template and can only contain letters, numbers and underscores. TEMPLATE_ID - The ID of the workflow's template, returned by tink template create . Options -r , --hardware - Required - Flag for the hardware data JSON mapping. -t , --template - Required - Flag for the template ID. -h , --help - Displays usage information for create . -f , --facility - string used to build grpc and http urls Examples tink workflow create -t aeedc9e2-6c1a-419f-976c-60f15803796f -r '{\"device_1\":\"08:00:27:00:00:01\"}' > Created Workflow: 3c5b71a7-0172-4d0d-bfff-6ca410b5697f tink workflow data Returns the ephemeral data for a specified workflow. tink workflow data <ID> Arguments ID - The ID of the workflow that you want the ephemeral data from. Options -l , --latest version - Returns the version number of the latest revision of the data. -m , --metadata - Returns the metadata only. -v , --version - Specify which version of the data to return. -h , --help - Displays usage information for delete . -f , --facility - string used to build grpc and http urls Examples tink workflow delete Deletes the specified workflow from the database. Doesn't return anything. tink workflow delete <ID> Arguments ID - The ID of the workflow that you want to delete. Use multiple IDs to delete more than one workflow at a time. Options -h , --help - Displays usage information for delete . -f , --facility - string used to build grpc and http urls Examples tink workflow delete 3c5b71a7-0172-4d0d-bfff-6ca410b5697f > tink workflow events Returns the events and status of actions that are performed as part of workflow execution. If the workflow has not been executed, it returns an empty table. tink workflow events <ID> Arguments ID - The ID of the workflow you want to see the events for. Options -h , --help - Displays usage information for events . -f , --facility - string used to build grpc and http urls Examples tink workflow events a8984b09-566d-47ba-b6c5-fbe482d8ad7f > +--------------------------------------+-------------+-------------+----------------+---------------------------------+--------------------+ | WORKER ID | TASK NAME | ACTION NAME | EXECUTION TIME | MESSAGE | ACTION STATUS | +--------------------------------------+-------------+-------------+----------------+---------------------------------+--------------------+ | ce2e62ed-826f-4485-a39f-a82bb74338e2 | hello world | hello_world | 0 | Started execution | ACTION_IN_PROGRESS | | ce2e62ed-826f-4485-a39f-a82bb74338e2 | hello world | hello_world | 0 | Finished Execution Successfully | ACTION_SUCCESS | +--------------------------------------+-------------+-------------+----------------+---------------------------------+--------------------+ tink workflow get Returns the workflow as it exists in the database. Useful for checking that the Worker's MAC or IP Address was correctly added at workflow creation. tink workflow get <ID> Arguments ID - The ID of the workflow you want to get. Options -h , --help - Displays usage information for get . -f , --facility - string used to build grpc and http urls Examples tink workflow get fe9a7798-19da-4968-be99-3f111aa789c0 > version: \"0.1\" name: hello_world_workflow global_timeout: 600 tasks: - name: \"hello world\" worker: \"08:00:27:00:00:01\" actions: - name: \"hello_world\" image: hello-world timeout: 60 tink workflow list Lists all the workflows currently stored in the database. tink workflow list <ID> [--help] [--facility] Options -h , --help - Displays usage information for get . -f , --facility - string used to build grpc and http urls Examples tink workflow list > +--------------------------------------+--------------------------------------+-----------------------------------+-------------------------------+-------------------------------+ | WORKFLOW ID | TEMPLATE ID | HARDWARE DEVICE | CREATED AT | UPDATED AT | +--------------------------------------+--------------------------------------+-----------------------------------+-------------------------------+-------------------------------+ | 7d81e2e3-5309-47d3-b4b0-8ba7bc02078b | c918a54a-5267-4efe-b884-849bcae9af65 | {\"device_1\": \"08:00:27:00:00:01\"} | 2020-07-08 15:17:02 +0000 UTC | 2020-07-08 15:17:02 +0000 UTC | +--------------------------------------+--------------------------------------+-----------------------------------+-------------------------------+-------------------------------+ tink workflow state Returns a summary of workflow status or progress. If run during workflow execution, it will return which task and action the workflow is currently working on. tink workflow state <ID> [--help] [--facility] Arguments ID - The ID of the workflow you want to get state information for. Options -h , --help - Displays usage information for state . -f , --facility - string used to build grpc and http urls Examples tink workflow state 7d81e2e3-5309-47d3-b4b0-8ba7bc02078b > +----------------------+--------------------------------------+ | FIELD NAME | VALUES | +----------------------+--------------------------------------+ | Workflow ID | 7d81e2e3-5309-47d3-b4b0-8ba7bc02078b | | Workflow Progress | 0% | | Current Task | | | Current Action | | | Current Worker | | | Current Action State | ACTION_PENDING | +----------------------+--------------------------------------+","title":"Workflow"},{"location":"cli-reference/workflow/#workflow-operations","text":"create Create a workflow. data Get the ephemeral data associated with a workflow. delete Delete a workflow. events Show all events for a workflow. get Get a workflow as it exists in the database. list List all the workflows in the database. state get the current workflow context tink workflow --help - Displays the available commands and usage for tink workflow .","title":"Workflow Operations"},{"location":"cli-reference/workflow/#tink-workflow-create","text":"Creates a workflow from a template ID and hardware data specified by MAC address or IP address. The workflow is stored in the database and the command returns a workflow ID. tink workflow create --hardware <MAC_ADDRESS or IP_ADDRESS> --template <TEMPLATE_ID> [--help] [--facility] Arguments MAC_ADDRESS or IP_ADDRESS - A JSON object containing the map of MAC or IP addresses of the hardware data used to identify the Worker (or workers) to their variables in the template. The key should match worker field in the template and can only contain letters, numbers and underscores. TEMPLATE_ID - The ID of the workflow's template, returned by tink template create . Options -r , --hardware - Required - Flag for the hardware data JSON mapping. -t , --template - Required - Flag for the template ID. -h , --help - Displays usage information for create . -f , --facility - string used to build grpc and http urls Examples tink workflow create -t aeedc9e2-6c1a-419f-976c-60f15803796f -r '{\"device_1\":\"08:00:27:00:00:01\"}' > Created Workflow: 3c5b71a7-0172-4d0d-bfff-6ca410b5697f","title":"tink workflow create"},{"location":"cli-reference/workflow/#tink-workflow-data","text":"Returns the ephemeral data for a specified workflow. tink workflow data <ID> Arguments ID - The ID of the workflow that you want the ephemeral data from. Options -l , --latest version - Returns the version number of the latest revision of the data. -m , --metadata - Returns the metadata only. -v , --version - Specify which version of the data to return. -h , --help - Displays usage information for delete . -f , --facility - string used to build grpc and http urls Examples","title":"tink workflow data"},{"location":"cli-reference/workflow/#tink-workflow-delete","text":"Deletes the specified workflow from the database. Doesn't return anything. tink workflow delete <ID> Arguments ID - The ID of the workflow that you want to delete. Use multiple IDs to delete more than one workflow at a time. Options -h , --help - Displays usage information for delete . -f , --facility - string used to build grpc and http urls Examples tink workflow delete 3c5b71a7-0172-4d0d-bfff-6ca410b5697f >","title":"tink workflow delete"},{"location":"cli-reference/workflow/#tink-workflow-events","text":"Returns the events and status of actions that are performed as part of workflow execution. If the workflow has not been executed, it returns an empty table. tink workflow events <ID> Arguments ID - The ID of the workflow you want to see the events for. Options -h , --help - Displays usage information for events . -f , --facility - string used to build grpc and http urls Examples tink workflow events a8984b09-566d-47ba-b6c5-fbe482d8ad7f > +--------------------------------------+-------------+-------------+----------------+---------------------------------+--------------------+ | WORKER ID | TASK NAME | ACTION NAME | EXECUTION TIME | MESSAGE | ACTION STATUS | +--------------------------------------+-------------+-------------+----------------+---------------------------------+--------------------+ | ce2e62ed-826f-4485-a39f-a82bb74338e2 | hello world | hello_world | 0 | Started execution | ACTION_IN_PROGRESS | | ce2e62ed-826f-4485-a39f-a82bb74338e2 | hello world | hello_world | 0 | Finished Execution Successfully | ACTION_SUCCESS | +--------------------------------------+-------------+-------------+----------------+---------------------------------+--------------------+","title":"tink workflow events"},{"location":"cli-reference/workflow/#tink-workflow-get","text":"Returns the workflow as it exists in the database. Useful for checking that the Worker's MAC or IP Address was correctly added at workflow creation. tink workflow get <ID> Arguments ID - The ID of the workflow you want to get. Options -h , --help - Displays usage information for get . -f , --facility - string used to build grpc and http urls Examples tink workflow get fe9a7798-19da-4968-be99-3f111aa789c0 > version: \"0.1\" name: hello_world_workflow global_timeout: 600 tasks: - name: \"hello world\" worker: \"08:00:27:00:00:01\" actions: - name: \"hello_world\" image: hello-world timeout: 60","title":"tink workflow get"},{"location":"cli-reference/workflow/#tink-workflow-list","text":"Lists all the workflows currently stored in the database. tink workflow list <ID> [--help] [--facility] Options -h , --help - Displays usage information for get . -f , --facility - string used to build grpc and http urls Examples tink workflow list > +--------------------------------------+--------------------------------------+-----------------------------------+-------------------------------+-------------------------------+ | WORKFLOW ID | TEMPLATE ID | HARDWARE DEVICE | CREATED AT | UPDATED AT | +--------------------------------------+--------------------------------------+-----------------------------------+-------------------------------+-------------------------------+ | 7d81e2e3-5309-47d3-b4b0-8ba7bc02078b | c918a54a-5267-4efe-b884-849bcae9af65 | {\"device_1\": \"08:00:27:00:00:01\"} | 2020-07-08 15:17:02 +0000 UTC | 2020-07-08 15:17:02 +0000 UTC | +--------------------------------------+--------------------------------------+-----------------------------------+-------------------------------+-------------------------------+","title":"tink workflow list"},{"location":"cli-reference/workflow/#tink-workflow-state","text":"Returns a summary of workflow status or progress. If run during workflow execution, it will return which task and action the workflow is currently working on. tink workflow state <ID> [--help] [--facility] Arguments ID - The ID of the workflow you want to get state information for. Options -h , --help - Displays usage information for state . -f , --facility - string used to build grpc and http urls Examples tink workflow state 7d81e2e3-5309-47d3-b4b0-8ba7bc02078b > +----------------------+--------------------------------------+ | FIELD NAME | VALUES | +----------------------+--------------------------------------+ | Workflow ID | 7d81e2e3-5309-47d3-b4b0-8ba7bc02078b | | Workflow Progress | 0% | | Current Task | | | Current Action | | | Current Worker | | | Current Action State | ACTION_PENDING | +----------------------+--------------------------------------+","title":"tink workflow state"},{"location":"examples/hello-world-workflow/","text":"A Hello-world Workflow The \"Hello World\" example uses an example hardware data, template, and workflow to show off some basic Tinkerbell functionality, and the interaction between a Provisioner and a Worker. It uses the hello-world docker image as an example task that could be performed on a Worker as part of a workflow. Prerequisites You have a Provisioner up and running, with the Tinkerbell stack installed and configured. This can be done locally with Vagrant as an experimental environment, on Packet for a more robust setup, or installed on any other environment that you have configured. You have a Worker that has not yet been brought up, or can be restarted. Hardware Data This example is intended to be environment agnostic, and assumes that you have a Worker machine as the intended target. The workflow in this example is simple enough that you can use the Minimal Hardware Data example with your targeted Worker's MAC Address and/or IP Address substituted in. The hello-world Action Image The workflow will have a single task that will have a single action. Actions are stored as images in an image repository either locally or remotely. For this example, pull down the hello-world image from Docker to host it locally in the Docker registry on the Provisioner. docker pull hello-world docker tag hello-world <registry-host>/hello-world docker push <registry-host>/hello-world This image doesn't have any instructions that the Worker will be able to perform, it's just an example to enable pushing a workflow out to the Worker when it comes up. The Template A template is a YAML file that lays out the series of tasks that you want to perform on the Worker after it boots up. The template for this workflow contains the one task with single hello-world action. The worker field contains a reference to device_1 which will be substituted with either the MAC Address or the IP Address of your Worker when you run the tink workflow create command in the next step. Save this template as hello-world.tmpl . version: \"0.1\" name: hello_world_workflow global_timeout: 600 tasks: - name: \"hello world\" worker: \"{{.device_1}}\" actions: - name: \"hello_world\" image: hello-world timeout: 60 The Workflow If you haven't already, be sure to have Pushed the Worker's hardware data to the database with tink hardware push . Created the template in the database with tink template create . You can now use the hardware data and the template to create a workflow. You need two pieces of information. The MAC Address or IP Address of your Worker as specified in the hardware data and the Template ID that is returned from the tink template create command. tink workflow create -t <template_id> -r '{\"device_1\": \"<MAC address/IP address>\"}' Workflow Execution You can now boot up or restart your Worker and a few things are going to happen: First, the Worker will iPXE boot into an Alpine Linux distribution running in memory Second, the Worker will call back to the Provisioner to check for it's workflow. Third, The Provisioner will push the workflow to the Worker for it to execute. While the workflow execution does not have much effect on the state of the Worker, you can check that the workflow was successfully executed from the tink workflow events command. tink workflow events <ID> > +--------------------------------------+-------------+-------------+----------------+---------------------------------+--------------------+ | WORKER ID | TASK NAME | ACTION NAME | EXECUTION TIME | MESSAGE | ACTION STATUS | +--------------------------------------+-------------+-------------+----------------+---------------------------------+--------------------+ | ce2e62ed-826f-4485-a39f-a82bb74338e2 | hello world | hello_world | 0 | Started execution | ACTION_IN_PROGRESS | | ce2e62ed-826f-4485-a39f-a82bb74338e2 | hello world | hello_world | 0 | Finished Execution Successfully | ACTION_SUCCESS | +--------------------------------------+-------------+-------------+----------------+---------------------------------+--------------------+ If you reboot the Worker at this point, it will again PXE boot, since no alternate operating system was installed as part of the hello-world workflow.","title":"A Hello-world Workflow"},{"location":"examples/hello-world-workflow/#a-hello-world-workflow","text":"The \"Hello World\" example uses an example hardware data, template, and workflow to show off some basic Tinkerbell functionality, and the interaction between a Provisioner and a Worker. It uses the hello-world docker image as an example task that could be performed on a Worker as part of a workflow.","title":"A Hello-world Workflow"},{"location":"examples/hello-world-workflow/#prerequisites","text":"You have a Provisioner up and running, with the Tinkerbell stack installed and configured. This can be done locally with Vagrant as an experimental environment, on Packet for a more robust setup, or installed on any other environment that you have configured. You have a Worker that has not yet been brought up, or can be restarted.","title":"Prerequisites"},{"location":"examples/hello-world-workflow/#hardware-data","text":"This example is intended to be environment agnostic, and assumes that you have a Worker machine as the intended target. The workflow in this example is simple enough that you can use the Minimal Hardware Data example with your targeted Worker's MAC Address and/or IP Address substituted in.","title":"Hardware Data"},{"location":"examples/hello-world-workflow/#the-hello-world-action-image","text":"The workflow will have a single task that will have a single action. Actions are stored as images in an image repository either locally or remotely. For this example, pull down the hello-world image from Docker to host it locally in the Docker registry on the Provisioner. docker pull hello-world docker tag hello-world <registry-host>/hello-world docker push <registry-host>/hello-world This image doesn't have any instructions that the Worker will be able to perform, it's just an example to enable pushing a workflow out to the Worker when it comes up.","title":"The hello-world Action Image"},{"location":"examples/hello-world-workflow/#the-template","text":"A template is a YAML file that lays out the series of tasks that you want to perform on the Worker after it boots up. The template for this workflow contains the one task with single hello-world action. The worker field contains a reference to device_1 which will be substituted with either the MAC Address or the IP Address of your Worker when you run the tink workflow create command in the next step. Save this template as hello-world.tmpl . version: \"0.1\" name: hello_world_workflow global_timeout: 600 tasks: - name: \"hello world\" worker: \"{{.device_1}}\" actions: - name: \"hello_world\" image: hello-world timeout: 60","title":"The Template"},{"location":"examples/hello-world-workflow/#the-workflow","text":"If you haven't already, be sure to have Pushed the Worker's hardware data to the database with tink hardware push . Created the template in the database with tink template create . You can now use the hardware data and the template to create a workflow. You need two pieces of information. The MAC Address or IP Address of your Worker as specified in the hardware data and the Template ID that is returned from the tink template create command. tink workflow create -t <template_id> -r '{\"device_1\": \"<MAC address/IP address>\"}'","title":"The Workflow"},{"location":"examples/hello-world-workflow/#workflow-execution","text":"You can now boot up or restart your Worker and a few things are going to happen: First, the Worker will iPXE boot into an Alpine Linux distribution running in memory Second, the Worker will call back to the Provisioner to check for it's workflow. Third, The Provisioner will push the workflow to the Worker for it to execute. While the workflow execution does not have much effect on the state of the Worker, you can check that the workflow was successfully executed from the tink workflow events command. tink workflow events <ID> > +--------------------------------------+-------------+-------------+----------------+---------------------------------+--------------------+ | WORKER ID | TASK NAME | ACTION NAME | EXECUTION TIME | MESSAGE | ACTION STATUS | +--------------------------------------+-------------+-------------+----------------+---------------------------------+--------------------+ | ce2e62ed-826f-4485-a39f-a82bb74338e2 | hello world | hello_world | 0 | Started execution | ACTION_IN_PROGRESS | | ce2e62ed-826f-4485-a39f-a82bb74338e2 | hello world | hello_world | 0 | Finished Execution Successfully | ACTION_SUCCESS | +--------------------------------------+-------------+-------------+----------------+---------------------------------+--------------------+ If you reboot the Worker at this point, it will again PXE boot, since no alternate operating system was installed as part of the hello-world workflow.","title":"Workflow Execution"},{"location":"services/boots/","text":"Boots Tinkerbell relies on network booting a server in order to prepare it to execute workflows. Boots is Tinkerbell's DHCP server, handling IP addresses and requests. It is also the TFTP server, serving iPXE and the initial installation image. Boots is written in Go, and can built, run, and tested outside of the Tinkerbell stack. Take a look at the code in its GitHub repository: tinkerbell/boots . What Boots does When a Worker comes on-line for the first time, it PXE boots and sends a DHCP request to the Provisioner. Boots receives the request and assigns the Worker its IP Address as defined in the hardware data. Next, Boots communicates over TFTP to download the iPXE script to the Worker. The iPXE script tells the Worker to download and boot an in-memory operating system called Osie . From there you are inside an OS and you can do what you like, the most common action is to partition your hard drive and installing the actual operating system. Tinkerbell abstracts those actions with the concept of a workflow. Other Resources One of the core concepts behind Tinkerbell is network booting. Let's imagine you are in a datacenter with hundreds of servers; it is not reasonable to go over all of the one by one with a USB stick to install the operating system you need. If you use Provisioner with an API, like Tinkerbell does, things get even more complicated as there isn't an operator running around with USB stick for every API request. There are a lot of articles and use cases for netbooting, here a few that our contributors enjoyed or even wrote: First journeys with netboot and ipxe installing Ubuntu The state of netbooting Raspberry Pis RedHat Enterprise Linux: PREPARING FOR A NETWORK INSTALLATION","title":"Boots"},{"location":"services/boots/#boots","text":"Tinkerbell relies on network booting a server in order to prepare it to execute workflows. Boots is Tinkerbell's DHCP server, handling IP addresses and requests. It is also the TFTP server, serving iPXE and the initial installation image. Boots is written in Go, and can built, run, and tested outside of the Tinkerbell stack. Take a look at the code in its GitHub repository: tinkerbell/boots .","title":"Boots"},{"location":"services/boots/#what-boots-does","text":"When a Worker comes on-line for the first time, it PXE boots and sends a DHCP request to the Provisioner. Boots receives the request and assigns the Worker its IP Address as defined in the hardware data. Next, Boots communicates over TFTP to download the iPXE script to the Worker. The iPXE script tells the Worker to download and boot an in-memory operating system called Osie . From there you are inside an OS and you can do what you like, the most common action is to partition your hard drive and installing the actual operating system. Tinkerbell abstracts those actions with the concept of a workflow.","title":"What Boots does"},{"location":"services/boots/#other-resources","text":"One of the core concepts behind Tinkerbell is network booting. Let's imagine you are in a datacenter with hundreds of servers; it is not reasonable to go over all of the one by one with a USB stick to install the operating system you need. If you use Provisioner with an API, like Tinkerbell does, things get even more complicated as there isn't an operator running around with USB stick for every API request. There are a lot of articles and use cases for netbooting, here a few that our contributors enjoyed or even wrote: First journeys with netboot and ipxe installing Ubuntu The state of netbooting Raspberry Pis RedHat Enterprise Linux: PREPARING FOR A NETWORK INSTALLATION","title":"Other Resources"},{"location":"services/hegel/","text":"Hegel GitHub repository: tinkerbell/hegel . Hegel is Tinkerbell's metadata store, supporting storage and retrieval of metadata over gRPC and HTTP. It also provides a compatible layer with the AWS EC2 metadata format. Metadata is user-defined as part of the hardware data that makes up a workflow. Using Hegel You can access Hegel in a Tinkerbell setup at the Provisioner's IP address /metadata . You can use cURL to retrieve the metadata it stores. Hegel by default exposes an HTTP API on port 50061 . You can interact with it via cURL curl <hegel_ip>:50061/metadata You can also retrieve a AWS EC2 compatible format uses from /meta-data . $ curl <hegel_ip>:50061/<date>/meta-data For example, if you are using the Vagrant Setup , Hegel runs as part of the Provisioner virtual machine with the IP: 192.168.1.2 . When the Worker starts and if you have logged in to osie using the password root you can access the metadata for your server via cURL : curl -s 192.168.1.2:50061/metadata | jq . > { \"facility\": { \"facility_code\": \"onprem\" }, \"instance\": {}, \"state\": \"\" } Or in AWS EC2 format: curl -s 192.168.1.2:50061/2009-04-04/meta-data If you look at the hardware-data.json that we used during the Vagrant setup you will find the facility_code=onprem as well. Other Resources Every cloud provider is capable of exposing metadata to servers that you can query as part of your automation, usually via HTTP. Some examples: AWS: Instance metadata and user data GCP: Storing and retrieving instance metadata Packet: Metadata","title":"Hegel"},{"location":"services/hegel/#hegel","text":"GitHub repository: tinkerbell/hegel . Hegel is Tinkerbell's metadata store, supporting storage and retrieval of metadata over gRPC and HTTP. It also provides a compatible layer with the AWS EC2 metadata format. Metadata is user-defined as part of the hardware data that makes up a workflow.","title":"Hegel"},{"location":"services/hegel/#using-hegel","text":"You can access Hegel in a Tinkerbell setup at the Provisioner's IP address /metadata . You can use cURL to retrieve the metadata it stores. Hegel by default exposes an HTTP API on port 50061 . You can interact with it via cURL curl <hegel_ip>:50061/metadata You can also retrieve a AWS EC2 compatible format uses from /meta-data . $ curl <hegel_ip>:50061/<date>/meta-data For example, if you are using the Vagrant Setup , Hegel runs as part of the Provisioner virtual machine with the IP: 192.168.1.2 . When the Worker starts and if you have logged in to osie using the password root you can access the metadata for your server via cURL : curl -s 192.168.1.2:50061/metadata | jq . > { \"facility\": { \"facility_code\": \"onprem\" }, \"instance\": {}, \"state\": \"\" } Or in AWS EC2 format: curl -s 192.168.1.2:50061/2009-04-04/meta-data If you look at the hardware-data.json that we used during the Vagrant setup you will find the facility_code=onprem as well.","title":"Using Hegel"},{"location":"services/hegel/#other-resources","text":"Every cloud provider is capable of exposing metadata to servers that you can query as part of your automation, usually via HTTP. Some examples: AWS: Instance metadata and user data GCP: Storing and retrieving instance metadata Packet: Metadata","title":"Other Resources"},{"location":"services/osie/","text":"OSIE When a Worker first starts in a Tinkerbell environment, it network boots and contacts the Provisioner where Boots handles its DHCP settings and provides iPXE support. As part of this process Tinkerbell sets up the Worker with an in-memory operating system called OSIE and is based on Alpine . OSIE is downloaded to the Worker via iPXE. Usually, this will happen when a Worker network boots and an operating system is not installed, otherwise the bootloader will boot from the disk which contains your operating system. OSIE can built, run, and tested outside of the Tinkerbell stack. Take a look at the code in its GitHub repository: tinkerbell/osie . Alpine and OSIE Alpine is known to be small (~130MB of storage) and easy to customize. The boot process is minimal and it does not require much configuration. OSIE builds on that to provide an environment capable of running the right actions required to configure your Worker and install a persistent operating system. OSIE gets compiled to a initial ramdisk and a kernel. The initial ramdisk contains Docker and it starts the tink-worker as a docker container. The tink-worker is the application that gets and manage workflows. Usually one of the first workflows it generates is the one that installs an operating system. Currently, the OSIE of today is a bit \"too fat\"! We are in the process of moving a lot of the customization out of OSIE and into workflows. When You See OSIE If you follow the Vagrant Setup tutorial you encounter OSIE at the section \"Start the Worker\". At that time the Worker has netbooted and Boots has pushed OSIE to the Worker, the workflow has started, and the Worker does not have an operating system yet. The Welcome to Alpine Linux 4.7 message comes from OSIE.","title":"OSIE"},{"location":"services/osie/#osie","text":"When a Worker first starts in a Tinkerbell environment, it network boots and contacts the Provisioner where Boots handles its DHCP settings and provides iPXE support. As part of this process Tinkerbell sets up the Worker with an in-memory operating system called OSIE and is based on Alpine . OSIE is downloaded to the Worker via iPXE. Usually, this will happen when a Worker network boots and an operating system is not installed, otherwise the bootloader will boot from the disk which contains your operating system. OSIE can built, run, and tested outside of the Tinkerbell stack. Take a look at the code in its GitHub repository: tinkerbell/osie .","title":"OSIE"},{"location":"services/osie/#alpine-and-osie","text":"Alpine is known to be small (~130MB of storage) and easy to customize. The boot process is minimal and it does not require much configuration. OSIE builds on that to provide an environment capable of running the right actions required to configure your Worker and install a persistent operating system. OSIE gets compiled to a initial ramdisk and a kernel. The initial ramdisk contains Docker and it starts the tink-worker as a docker container. The tink-worker is the application that gets and manage workflows. Usually one of the first workflows it generates is the one that installs an operating system. Currently, the OSIE of today is a bit \"too fat\"! We are in the process of moving a lot of the customization out of OSIE and into workflows.","title":"Alpine and OSIE"},{"location":"services/osie/#when-you-see-osie","text":"If you follow the Vagrant Setup tutorial you encounter OSIE at the section \"Start the Worker\". At that time the Worker has netbooted and Boots has pushed OSIE to the Worker, the workflow has started, and the Worker does not have an operating system yet. The Welcome to Alpine Linux 4.7 message comes from OSIE.","title":"When You See OSIE"},{"location":"services/tink/","text":"Tink Tink provides the user interface and the API gateway to expose all the features distributed across the various other Tinkerbell services. It lives in the GitHub repository: tinkerbell/tink . It exposes three binaries: The tink-server is a long running daemon written in Go that exposes a gRPC API. As a user and operator this is your entry point. You can register new hardware, create templates and workflows, and much more. The tink-cli is one of the way you can use to interact with the tink-server . It is a command line interface written in Go and Cobra . The tink-worker is a binary that runs in every worker machine. It is one of the first processes started by a Worker and it executes workflows. Getting Tink Right now we do not yet have a release cycle in place that builds and release binaries. You can either compile them by yourself or you can use the Docker container that is already built. The docker containers are the ones in use when you follow the setup tutorial and run docker-compose . Getting the Docker Images We relay on Docker a lot for both code distribution but and workflow execution. Our CI/CD pipeline builds and pushes images to quay.io a popular image repository similar to Docker Hub. There is a repository for every tool: tink-cli tink-worker tink-server The tags are composed as: sha-<gitsha> , where gitsha is the first 7 characters of a git commit. Only master commits are pushed to quay.io. Building the Binaries Tinkerbell uses the standard Golang toolchain. You can clone the tink repository: git clone git@github.com:tinkerbell/tink All the binaries are inside cmd/tink-* . Based on what you need, you can run go build . For example if you would like to compile the CLI, run: go build cmd/tink-cli/main.go You can also use go run if you want to run code without having to compile a binary: go run cmd/tink-server/main.go Building and Running tink-cli One use case of the binaries, is if you want to run the tink-cli binary on the Provisioner, outside the tink-server container. This simplifies the CLI command usage. Prerequisites: A bit of familiarity with go build , and Go has to be installed. A Provisioner up and running Tinkerbell (works with the Vagrant setup, for example) SSH into the Provisioner and Navigate to the directory where you have cloned the tink repository. ssh cd tink Now let's compile the binary with: $ go build -o tink cmd/tink-cli/main.go All the traffic between Tinkerbell services is encrypted via TLS, so before running any tink commands there are two environment variables that authenticate the CLI to the tink-server . The tink-server entry point is 127.0.0.1 and exposes ports 42113 and 42114. TINKERBELL_CERT_URL=http://127.0.0.1:42114/cert TINKERBELL_GRPC_AUTHORITY=127.0.0.1:42113 NOTE: In a real environment every person that as access to the host and ports can authenticate and use tink-server . You can export them as environment variables or you can run them in-line as part of the tink command. $ export TINKERBELL_GRPC_AUTHORITY=127.0.0.1:42113 $ export TINKERBELL_CERT_URL=http://127.0.0.1:42114/cert Now you can run tink commands without docker-exec . $ tink hardware list > +----+-------------+------------+----------+ | ID | MAC ADDRESS | IP ADDRESS | HOSTNAME | +----+-------------+------------+----------+ +----+-------------+------------+----------+ You can also test by making some hardware data. $ cat > hardware-data.json <<EOF { \"id\": \"ce2e62ed-826f-4485-a39f-a82bb74338e2\", \"metadata\": { \"facility\": { \"facility_code\": \"onprem\" }, \"instance\": {}, \"state\": \"\" }, \"network\": { \"interfaces\": [ { \"dhcp\": { \"arch\": \"x86_64\", \"ip\": { \"address\": \"192.168.1.5\", \"gateway\": \"192.168.1.1\", \"netmask\": \"255.255.255.248\" }, \"mac\": \"08:00:27:00:00:01\", \"uefi\": false }, \"netboot\": { \"allow_pxe\": true, \"allow_workflow\": true } } ] } } EOF tink hardware push < ./hardware-data.json > 2020/08/31 10:20:20 Hardware data pushed successfully","title":"Tink"},{"location":"services/tink/#tink","text":"Tink provides the user interface and the API gateway to expose all the features distributed across the various other Tinkerbell services. It lives in the GitHub repository: tinkerbell/tink . It exposes three binaries: The tink-server is a long running daemon written in Go that exposes a gRPC API. As a user and operator this is your entry point. You can register new hardware, create templates and workflows, and much more. The tink-cli is one of the way you can use to interact with the tink-server . It is a command line interface written in Go and Cobra . The tink-worker is a binary that runs in every worker machine. It is one of the first processes started by a Worker and it executes workflows.","title":"Tink"},{"location":"services/tink/#getting-tink","text":"Right now we do not yet have a release cycle in place that builds and release binaries. You can either compile them by yourself or you can use the Docker container that is already built. The docker containers are the ones in use when you follow the setup tutorial and run docker-compose .","title":"Getting Tink"},{"location":"services/tink/#getting-the-docker-images","text":"We relay on Docker a lot for both code distribution but and workflow execution. Our CI/CD pipeline builds and pushes images to quay.io a popular image repository similar to Docker Hub. There is a repository for every tool: tink-cli tink-worker tink-server The tags are composed as: sha-<gitsha> , where gitsha is the first 7 characters of a git commit. Only master commits are pushed to quay.io.","title":"Getting the Docker Images"},{"location":"services/tink/#building-the-binaries","text":"Tinkerbell uses the standard Golang toolchain. You can clone the tink repository: git clone git@github.com:tinkerbell/tink All the binaries are inside cmd/tink-* . Based on what you need, you can run go build . For example if you would like to compile the CLI, run: go build cmd/tink-cli/main.go You can also use go run if you want to run code without having to compile a binary: go run cmd/tink-server/main.go","title":"Building the Binaries"},{"location":"services/tink/#building-and-running-tink-cli","text":"One use case of the binaries, is if you want to run the tink-cli binary on the Provisioner, outside the tink-server container. This simplifies the CLI command usage. Prerequisites: A bit of familiarity with go build , and Go has to be installed. A Provisioner up and running Tinkerbell (works with the Vagrant setup, for example) SSH into the Provisioner and Navigate to the directory where you have cloned the tink repository. ssh cd tink Now let's compile the binary with: $ go build -o tink cmd/tink-cli/main.go All the traffic between Tinkerbell services is encrypted via TLS, so before running any tink commands there are two environment variables that authenticate the CLI to the tink-server . The tink-server entry point is 127.0.0.1 and exposes ports 42113 and 42114. TINKERBELL_CERT_URL=http://127.0.0.1:42114/cert TINKERBELL_GRPC_AUTHORITY=127.0.0.1:42113 NOTE: In a real environment every person that as access to the host and ports can authenticate and use tink-server . You can export them as environment variables or you can run them in-line as part of the tink command. $ export TINKERBELL_GRPC_AUTHORITY=127.0.0.1:42113 $ export TINKERBELL_CERT_URL=http://127.0.0.1:42114/cert Now you can run tink commands without docker-exec . $ tink hardware list > +----+-------------+------------+----------+ | ID | MAC ADDRESS | IP ADDRESS | HOSTNAME | +----+-------------+------------+----------+ +----+-------------+------------+----------+ You can also test by making some hardware data. $ cat > hardware-data.json <<EOF { \"id\": \"ce2e62ed-826f-4485-a39f-a82bb74338e2\", \"metadata\": { \"facility\": { \"facility_code\": \"onprem\" }, \"instance\": {}, \"state\": \"\" }, \"network\": { \"interfaces\": [ { \"dhcp\": { \"arch\": \"x86_64\", \"ip\": { \"address\": \"192.168.1.5\", \"gateway\": \"192.168.1.1\", \"netmask\": \"255.255.255.248\" }, \"mac\": \"08:00:27:00:00:01\", \"uefi\": false }, \"netboot\": { \"allow_pxe\": true, \"allow_workflow\": true } } ] } } EOF tink hardware push < ./hardware-data.json > 2020/08/31 10:20:20 Hardware data pushed successfully","title":"Building and Running tink-cli"},{"location":"setup/extending-vagrant/","text":"Extending the Vagrant Setup If you have followed the guide to getting Vagrant set up locally, you might be interested in other things you can do with it. There are some steps that you may need to take in order to make the setup a bit more functional. Getting Your Worker Connected to the Internet A Worker machine created by Vagrant is not Internet facing because the newly deployed host uses the tink server as its network gateway. If you are looking to develop more workflows, it might be useful to allow the Worker access to the Internet to do things like run apt-get update or pull down images directly from Docker. In order to provide this functionality, you need to configure iptables so the network gateway can speak to the Internet, allowing the Worker machine Internet access. SSH into the Provisioner. Set the environment variables of the two network interfaces: export main=<public_ip_iface> export vagrant=<tinkerbell_ip_iface> Enable Internet forwarding on the tink server. (Note: This is not permanent, you would need to edit /etc/sysctl to make it permanent.) echo 1 > /proc/sys/net/ipv4/ip_forward Forward traffic from the vagrant network to main interface. iptables -A FORWARD -i $vagrant -o $main -j ACCEPT Forward the existing traffic back from the main interface to the vagrant network. iptables -A FORWARD -i $main -o $vagrant -m state --state ESTABLISHED,RELATED \\ -j ACCEPT Translate addresses so traffic appears to have come from the correct address. iptables -t nat -A POSTROUTING -o $vagrant -j MASQUERADE Running Tests with Vagrant If you are developing on Tinkerbell, it might be handy to know that the Vagrant setup serves as the backbone of some of the end-to-end testing. The scripts that set up and run the tests are in the tink repository, in the test/_vagrant directory. The requirements for the tests are the same as the Vagrant setup itself, along with Go installed on your local machine. To run the tests, run the go test command, pointed at the test/_vagrant directory. go test ./test/_vagrant/... Reusing osie.tar.gz While playing with Tinkerbell locally, it becomes a pain to download osie.tar.gz as part of the provisioner setup each time you recreate the stack. However, we can skip the download and resuse existing osie.tar.gz by setting the TB_OSIE_TAR environment variable. Check setup.sh for reference. Download Osie before starting the setup curl https://tinkerbell-oss.s3.amazonaws.com/osie-uploads/latest.tar.gz -o osie.tar.gz Move the downloaded file to tink/deploy/vagrant/ . Now, set the environment variable in tink/deploy/vagrant/scripts/tinkerbell.sh before it executes setup.sh . ... export TB_OSIE_TAR='/vagrant/deploy/vagrant/osie.tar.gz' ./setup.sh ... Start the vagrant setup.","title":"Extending the Vagrant Setup"},{"location":"setup/extending-vagrant/#extending-the-vagrant-setup","text":"If you have followed the guide to getting Vagrant set up locally, you might be interested in other things you can do with it. There are some steps that you may need to take in order to make the setup a bit more functional.","title":"Extending the Vagrant Setup"},{"location":"setup/extending-vagrant/#getting-your-worker-connected-to-the-internet","text":"A Worker machine created by Vagrant is not Internet facing because the newly deployed host uses the tink server as its network gateway. If you are looking to develop more workflows, it might be useful to allow the Worker access to the Internet to do things like run apt-get update or pull down images directly from Docker. In order to provide this functionality, you need to configure iptables so the network gateway can speak to the Internet, allowing the Worker machine Internet access. SSH into the Provisioner. Set the environment variables of the two network interfaces: export main=<public_ip_iface> export vagrant=<tinkerbell_ip_iface> Enable Internet forwarding on the tink server. (Note: This is not permanent, you would need to edit /etc/sysctl to make it permanent.) echo 1 > /proc/sys/net/ipv4/ip_forward Forward traffic from the vagrant network to main interface. iptables -A FORWARD -i $vagrant -o $main -j ACCEPT Forward the existing traffic back from the main interface to the vagrant network. iptables -A FORWARD -i $main -o $vagrant -m state --state ESTABLISHED,RELATED \\ -j ACCEPT Translate addresses so traffic appears to have come from the correct address. iptables -t nat -A POSTROUTING -o $vagrant -j MASQUERADE","title":"Getting Your Worker Connected to the Internet"},{"location":"setup/extending-vagrant/#running-tests-with-vagrant","text":"If you are developing on Tinkerbell, it might be handy to know that the Vagrant setup serves as the backbone of some of the end-to-end testing. The scripts that set up and run the tests are in the tink repository, in the test/_vagrant directory. The requirements for the tests are the same as the Vagrant setup itself, along with Go installed on your local machine. To run the tests, run the go test command, pointed at the test/_vagrant directory. go test ./test/_vagrant/...","title":"Running Tests with Vagrant"},{"location":"setup/extending-vagrant/#reusing-osietargz","text":"While playing with Tinkerbell locally, it becomes a pain to download osie.tar.gz as part of the provisioner setup each time you recreate the stack. However, we can skip the download and resuse existing osie.tar.gz by setting the TB_OSIE_TAR environment variable. Check setup.sh for reference. Download Osie before starting the setup curl https://tinkerbell-oss.s3.amazonaws.com/osie-uploads/latest.tar.gz -o osie.tar.gz Move the downloaded file to tink/deploy/vagrant/ . Now, set the environment variable in tink/deploy/vagrant/scripts/tinkerbell.sh before it executes setup.sh . ... export TB_OSIE_TAR='/vagrant/deploy/vagrant/osie.tar.gz' ./setup.sh ... Start the vagrant setup.","title":"Reusing osie.tar.gz"},{"location":"setup/local-vagrant/","text":"Local Setup with Vagrant If you want to dive in to trying out Tinkerbell, this tutorial sets it up locally using Vagrant. Vagrant manages the Tinkerbell installation for this tutorial's Provisioner, and runs both the Provisioner and Worker on VirtualBox or libvirtd . It covers some basic aspects of Tinkerbell's functionality: setting up a Provisioner creating the hardware data for the Worker creating a template with a placeholder action item, using the hello-world example and creating a workflow The last step is to start up the Worker, which will call back to the Provisioner for its workflow. Prerequisites The host's processor should support virtualization Vagrant is installed Either VirtualBox or libvirtd is installed. Getting Tinkerbell To get Tinkerbell, clone the tink repository. git clone https://github.com/tinkerbell/tink.git Move into the deploy/vagrant directory. This folder contains a Vagrant configuration file (Vagrantfile) needed to setup the Provisioner and the Worker. cd tink/deploy/vagrant Start the Provisioner Since Vagrant is handling the Provisioner's configuration, including installing the Tinkerbell stack, run the command to start it up. vagrant up provisioner > Bringing machine 'provisioner' up with 'virtualbox' provider... The Provisioner installs and runs Ubuntu with a couple of additional utilities. The time it takes to spin up the Provisioner varies with connection speed and resources on your local machine. When the Provisioner is ready, you should see the following message: INFO: tinkerbell stack setup completed successfully on ubuntu server Starting Tinkerbell Now that the Provisioner's machine is up and running, you can connect and bring up Tinkerbell. SSH into the Provisioner. vagrant ssh provisioner > vagrant@provisioner:~$ Tinkerbell is going to be running from a container, so navigate to the vagrant directory, set the environment, and start the Tinkerbell stack with docker-compose . cd /vagrant && source envrc && cd deploy docker-compose up -d The Tinkerbell server, and more importantly the CLI, are now managed like a standard Docker Compose project. Just make sure to have sourced the envrc before issuing docker-compose commands. Tinkerbell is now ready to receive templates and workflows. Check out all the Tinkerbell services are running. docker-compose ps > Name Command State Ports ------------------------------------------------------------------------------------------------------------------------- deploy_boots_1 /boots -dhcp-addr 0.0.0.0: ... Up deploy_cacher_1 /cacher Up 0.0.0.0:42111->42111/tcp, 0.0.0.0:42112->42112/tcp deploy_db_1 docker-entrypoint.sh postgres Up (healthy) 0.0.0.0:5432->5432/tcp deploy_hegel_1 cmd/hegel Up deploy_nginx_1 /docker-entrypoint.sh ngin ... Up 192.168.1.2:80->80/tcp deploy_registry_1 /entrypoint.sh /etc/docker ... Up (healthy) deploy_tink-cli_1 /bin/sh -c sleep infinity Up deploy_tink-server_1 tink-server Up (healthy) 0.0.0.0:42113->42113/tcp, 0.0.0.0:42114->42114/tcp At this point, you might want to open a ssh connection to show logs from the Provisioner, because it will show what the tink-server is doing through the rest of the setup. Open a new terminal, ssh in to the provisioner as you did before, and run docker-compose logs to tail logs. cd tink/deploy/vagrant vagrant ssh provisioner cd /vagrant && source envrc && cd deploy docker-compose logs -f tink-server boots nginx Later in the tutorial you can check the logs from tink-server in order to see the execution of the workflow. The last step for the Provisioner to do at this point is to pull down the image that will be used in the workflow. Tinkerbell uses Docker registry to host images locally, so pull down the \"Hello World\" docker image and push it to the registry. docker pull hello-world docker tag hello-world 192.168.1.1/hello-world docker push 192.168.1.1/hello-world Creating the Worker's Hardware Data With the provisioner up and running, it's time to set up the worker's configuration. First, define the Worker's hardware data, which is used to identify the Worker as the target of a workflow. Very minimal hardware data is required for this example, but it does at least need to contain the MAC Address of the Worker, which is hardcoded in the Vagrant file, and have the Worker set to allow PXE booting and accept workflows. cat > hardware-data.json <<EOF { \"id\": \"ce2e62ed-826f-4485-a39f-a82bb74338e2\", \"metadata\": { \"facility\": { \"facility_code\": \"onprem\" }, \"instance\": {}, \"state\": \"\" }, \"network\": { \"interfaces\": [ { \"dhcp\": { \"arch\": \"x86_64\", \"ip\": { \"address\": \"192.168.1.5\", \"gateway\": \"192.168.1.1\", \"netmask\": \"255.255.255.248\" }, \"mac\": \"08:00:27:00:00:01\", \"uefi\": false }, \"netboot\": { \"allow_pxe\": true, \"allow_workflow\": true } } ] } } EOF Then, push the hardware data to the database with the tink hardware push command. docker exec -i deploy_tink-cli_1 tink hardware push < ./hardware-data.json > 2020/06/17 14:12:45 Hardware data pushed successfully If you are following along in the tink-server logs, you should see: tink-server_1 | {\"level\":\"info\",\"ts\":1592936402.3975577,\"caller\":\"grpc-server/hardware.go:37\",\"msg\":\"data pushed\",\"service\":\"github.com/tinkerbell/tink\",\"id\":\"ce2e62ed-826f-4485-a39f-a82bb74338e2\"} Creating a Template Next, define the template for the workflow. The template sets out tasks for the Worker to preform sequentially. This template contains a single task with a single action, which is to perform \"hello-world\". Just as in the hello-world example , the \"hello-world\" image doesn't contain any instructions that the Worker will perform. It is just a placeholder in the template so a workflow can be created and pushed to the Worker. cat > hello-world.yml <<EOF version: \"0.1\" name: hello_world_workflow global_timeout: 600 tasks: - name: \"hello world\" worker: \"{{.device_1}}\" actions: - name: \"hello_world\" image: hello-world timeout: 60 EOF Create the template and push it to the database with the tink template create command. docker exec -i deploy_tink-cli_1 tink template create --name hello-world < ./hello-world.yml > Created Template: 75ab8483-6f42-42a9-a80d-a9f6196130df The command returns a Template ID, and if you are watching the tink-server logs you will see: tink-server_1 | {\"level\":\"info\",\"ts\":1592934670.2717152,\"caller\":\"grpc-server/template.go:34\",\"msg\":\"done creating a new Template\",\"service\":\"github.com/tinkerbell/tink\"} Creating the Workflow The next step is to combine both the hardware data and the template to create a workflow. First, the workflow needs to know which template to execute. The Template ID you should use was returned by tink template create command executed above. Second, the Workflow needs a target, defined by the hardware data. In this example, the target is identified by a MAC address set in the hardware data for our Worker, so 08:00:27:00:00:01 . (Note: this MAC address is hard coded in the Vagrantfile.) Combine these two pieces of information and create the workflow with the tink workflow create command. docker exec -i deploy_tink-cli_1 tink workflow create \\ -t <TEMPLATE ID> \\ -r '{\"device_1\":\"08:00:27:00:00:01\"}' > Created Workflow: a8984b09-566d-47ba-b6c5-fbe482d8ad7f The command returns a Workflow ID and if you are watching the logs, you will see: tink-server_1 | {\"level\":\"info\",\"ts\":1592936829.6773047,\"caller\":\"grpc-server/workflow.go:63\",\"msg\":\"done creating a new workflow\",\"service\":\"github.com/tinkerbell/tink\"} Start the Worker You can now bring up the Worker and execute the Workflow. In a new terminal window, move into the tink/deploy/vagrant directory, and bring up the Worker with Vagrant, similar to bringing up the Provisioner. cd tink/deploy/vagrant vagrant up worker If you are using VirtualBox, it will bring up a UI, and after the setup, you will see a login screen. You can login with the username root and no password is required. Tinkerbell will netboot a custom AlpineOS that runs in RAM, so any changes you make won't be persisted between reboots. Note: If you have a high-resolution monitor, here are a few notes about how to make the UI bigger . At this point you should check on the Provisioner to confirm that the Workflow was executed on the Worker. If you opened a terminal window to monitor the Tinkerbell logs, you should see the execution in them. tink-server_1 | Received action status: workflow_id:\"a8984b09-566d-47ba-b6c5-fbe482d8ad7f\" task_name:\"hello world\" action_name:\"hello_world\" action_status:ACTION_SUCCESS message:\"Finished Execution Successfully\" worker_id:\"ce2e62ed-826f-4485-a39f-a82bb74338e2\" tink-server_1 | Current context workflow_id:\"a8984b09-566d-47ba-b6c5-fbe482d8ad7f\" current_worker:\"ce2e62ed-826f-4485-a39f-a82bb74338e2\" current_task:\"hello world\" current_action:\"hello_world\" current_action_state:ACTION_SUCCESS total_number_of_actions:1 You can also check using the tink workflow events and the Workflow ID on the Provisioner. docker exec -i deploy_tink-cli_1 tink workflow events a8984b09-566d-47ba-b6c5-fbe482d8ad7f > +--------------------------------------+-------------+-------------+----------------+---------------------------------+--------------------+ | WORKER ID | TASK NAME | ACTION NAME | EXECUTION TIME | MESSAGE | ACTION STATUS | +--------------------------------------+-------------+-------------+----------------+---------------------------------+--------------------+ | ce2e62ed-826f-4485-a39f-a82bb74338e2 | hello world | hello_world | 0 | Started execution | ACTION_IN_PROGRESS | | ce2e62ed-826f-4485-a39f-a82bb74338e2 | hello world | hello_world | 0 | Finished Execution Successfully | ACTION_SUCCESS | +--------------------------------------+-------------+-------------+----------------+---------------------------------+--------------------+ Summary Getting set up locally is a good way to sample Tinkerbell's functionality. The Vagrant set up is not necessarily intended to be persistent, but while it's up and running, you can use the Provisioner to test out the CLI commands or just explore the stack. If you are looking to extend your local setup to develop or test out other workflows, check out the Extending the Vagrant Setup doc. That's it! Let us know what you think about it on Slack .","title":"Local Setup with Vagrant"},{"location":"setup/local-vagrant/#local-setup-with-vagrant","text":"If you want to dive in to trying out Tinkerbell, this tutorial sets it up locally using Vagrant. Vagrant manages the Tinkerbell installation for this tutorial's Provisioner, and runs both the Provisioner and Worker on VirtualBox or libvirtd . It covers some basic aspects of Tinkerbell's functionality: setting up a Provisioner creating the hardware data for the Worker creating a template with a placeholder action item, using the hello-world example and creating a workflow The last step is to start up the Worker, which will call back to the Provisioner for its workflow.","title":"Local Setup with Vagrant"},{"location":"setup/local-vagrant/#prerequisites","text":"The host's processor should support virtualization Vagrant is installed Either VirtualBox or libvirtd is installed.","title":"Prerequisites"},{"location":"setup/local-vagrant/#getting-tinkerbell","text":"To get Tinkerbell, clone the tink repository. git clone https://github.com/tinkerbell/tink.git Move into the deploy/vagrant directory. This folder contains a Vagrant configuration file (Vagrantfile) needed to setup the Provisioner and the Worker. cd tink/deploy/vagrant","title":"Getting Tinkerbell"},{"location":"setup/local-vagrant/#start-the-provisioner","text":"Since Vagrant is handling the Provisioner's configuration, including installing the Tinkerbell stack, run the command to start it up. vagrant up provisioner > Bringing machine 'provisioner' up with 'virtualbox' provider... The Provisioner installs and runs Ubuntu with a couple of additional utilities. The time it takes to spin up the Provisioner varies with connection speed and resources on your local machine. When the Provisioner is ready, you should see the following message: INFO: tinkerbell stack setup completed successfully on ubuntu server","title":"Start the Provisioner"},{"location":"setup/local-vagrant/#starting-tinkerbell","text":"Now that the Provisioner's machine is up and running, you can connect and bring up Tinkerbell. SSH into the Provisioner. vagrant ssh provisioner > vagrant@provisioner:~$ Tinkerbell is going to be running from a container, so navigate to the vagrant directory, set the environment, and start the Tinkerbell stack with docker-compose . cd /vagrant && source envrc && cd deploy docker-compose up -d The Tinkerbell server, and more importantly the CLI, are now managed like a standard Docker Compose project. Just make sure to have sourced the envrc before issuing docker-compose commands. Tinkerbell is now ready to receive templates and workflows. Check out all the Tinkerbell services are running. docker-compose ps > Name Command State Ports ------------------------------------------------------------------------------------------------------------------------- deploy_boots_1 /boots -dhcp-addr 0.0.0.0: ... Up deploy_cacher_1 /cacher Up 0.0.0.0:42111->42111/tcp, 0.0.0.0:42112->42112/tcp deploy_db_1 docker-entrypoint.sh postgres Up (healthy) 0.0.0.0:5432->5432/tcp deploy_hegel_1 cmd/hegel Up deploy_nginx_1 /docker-entrypoint.sh ngin ... Up 192.168.1.2:80->80/tcp deploy_registry_1 /entrypoint.sh /etc/docker ... Up (healthy) deploy_tink-cli_1 /bin/sh -c sleep infinity Up deploy_tink-server_1 tink-server Up (healthy) 0.0.0.0:42113->42113/tcp, 0.0.0.0:42114->42114/tcp At this point, you might want to open a ssh connection to show logs from the Provisioner, because it will show what the tink-server is doing through the rest of the setup. Open a new terminal, ssh in to the provisioner as you did before, and run docker-compose logs to tail logs. cd tink/deploy/vagrant vagrant ssh provisioner cd /vagrant && source envrc && cd deploy docker-compose logs -f tink-server boots nginx Later in the tutorial you can check the logs from tink-server in order to see the execution of the workflow. The last step for the Provisioner to do at this point is to pull down the image that will be used in the workflow. Tinkerbell uses Docker registry to host images locally, so pull down the \"Hello World\" docker image and push it to the registry. docker pull hello-world docker tag hello-world 192.168.1.1/hello-world docker push 192.168.1.1/hello-world","title":"Starting Tinkerbell"},{"location":"setup/local-vagrant/#creating-the-workers-hardware-data","text":"With the provisioner up and running, it's time to set up the worker's configuration. First, define the Worker's hardware data, which is used to identify the Worker as the target of a workflow. Very minimal hardware data is required for this example, but it does at least need to contain the MAC Address of the Worker, which is hardcoded in the Vagrant file, and have the Worker set to allow PXE booting and accept workflows. cat > hardware-data.json <<EOF { \"id\": \"ce2e62ed-826f-4485-a39f-a82bb74338e2\", \"metadata\": { \"facility\": { \"facility_code\": \"onprem\" }, \"instance\": {}, \"state\": \"\" }, \"network\": { \"interfaces\": [ { \"dhcp\": { \"arch\": \"x86_64\", \"ip\": { \"address\": \"192.168.1.5\", \"gateway\": \"192.168.1.1\", \"netmask\": \"255.255.255.248\" }, \"mac\": \"08:00:27:00:00:01\", \"uefi\": false }, \"netboot\": { \"allow_pxe\": true, \"allow_workflow\": true } } ] } } EOF Then, push the hardware data to the database with the tink hardware push command. docker exec -i deploy_tink-cli_1 tink hardware push < ./hardware-data.json > 2020/06/17 14:12:45 Hardware data pushed successfully If you are following along in the tink-server logs, you should see: tink-server_1 | {\"level\":\"info\",\"ts\":1592936402.3975577,\"caller\":\"grpc-server/hardware.go:37\",\"msg\":\"data pushed\",\"service\":\"github.com/tinkerbell/tink\",\"id\":\"ce2e62ed-826f-4485-a39f-a82bb74338e2\"}","title":"Creating the Worker's Hardware Data"},{"location":"setup/local-vagrant/#creating-a-template","text":"Next, define the template for the workflow. The template sets out tasks for the Worker to preform sequentially. This template contains a single task with a single action, which is to perform \"hello-world\". Just as in the hello-world example , the \"hello-world\" image doesn't contain any instructions that the Worker will perform. It is just a placeholder in the template so a workflow can be created and pushed to the Worker. cat > hello-world.yml <<EOF version: \"0.1\" name: hello_world_workflow global_timeout: 600 tasks: - name: \"hello world\" worker: \"{{.device_1}}\" actions: - name: \"hello_world\" image: hello-world timeout: 60 EOF Create the template and push it to the database with the tink template create command. docker exec -i deploy_tink-cli_1 tink template create --name hello-world < ./hello-world.yml > Created Template: 75ab8483-6f42-42a9-a80d-a9f6196130df The command returns a Template ID, and if you are watching the tink-server logs you will see: tink-server_1 | {\"level\":\"info\",\"ts\":1592934670.2717152,\"caller\":\"grpc-server/template.go:34\",\"msg\":\"done creating a new Template\",\"service\":\"github.com/tinkerbell/tink\"}","title":"Creating a Template"},{"location":"setup/local-vagrant/#creating-the-workflow","text":"The next step is to combine both the hardware data and the template to create a workflow. First, the workflow needs to know which template to execute. The Template ID you should use was returned by tink template create command executed above. Second, the Workflow needs a target, defined by the hardware data. In this example, the target is identified by a MAC address set in the hardware data for our Worker, so 08:00:27:00:00:01 . (Note: this MAC address is hard coded in the Vagrantfile.) Combine these two pieces of information and create the workflow with the tink workflow create command. docker exec -i deploy_tink-cli_1 tink workflow create \\ -t <TEMPLATE ID> \\ -r '{\"device_1\":\"08:00:27:00:00:01\"}' > Created Workflow: a8984b09-566d-47ba-b6c5-fbe482d8ad7f The command returns a Workflow ID and if you are watching the logs, you will see: tink-server_1 | {\"level\":\"info\",\"ts\":1592936829.6773047,\"caller\":\"grpc-server/workflow.go:63\",\"msg\":\"done creating a new workflow\",\"service\":\"github.com/tinkerbell/tink\"}","title":"Creating the Workflow"},{"location":"setup/local-vagrant/#start-the-worker","text":"You can now bring up the Worker and execute the Workflow. In a new terminal window, move into the tink/deploy/vagrant directory, and bring up the Worker with Vagrant, similar to bringing up the Provisioner. cd tink/deploy/vagrant vagrant up worker If you are using VirtualBox, it will bring up a UI, and after the setup, you will see a login screen. You can login with the username root and no password is required. Tinkerbell will netboot a custom AlpineOS that runs in RAM, so any changes you make won't be persisted between reboots. Note: If you have a high-resolution monitor, here are a few notes about how to make the UI bigger . At this point you should check on the Provisioner to confirm that the Workflow was executed on the Worker. If you opened a terminal window to monitor the Tinkerbell logs, you should see the execution in them. tink-server_1 | Received action status: workflow_id:\"a8984b09-566d-47ba-b6c5-fbe482d8ad7f\" task_name:\"hello world\" action_name:\"hello_world\" action_status:ACTION_SUCCESS message:\"Finished Execution Successfully\" worker_id:\"ce2e62ed-826f-4485-a39f-a82bb74338e2\" tink-server_1 | Current context workflow_id:\"a8984b09-566d-47ba-b6c5-fbe482d8ad7f\" current_worker:\"ce2e62ed-826f-4485-a39f-a82bb74338e2\" current_task:\"hello world\" current_action:\"hello_world\" current_action_state:ACTION_SUCCESS total_number_of_actions:1 You can also check using the tink workflow events and the Workflow ID on the Provisioner. docker exec -i deploy_tink-cli_1 tink workflow events a8984b09-566d-47ba-b6c5-fbe482d8ad7f > +--------------------------------------+-------------+-------------+----------------+---------------------------------+--------------------+ | WORKER ID | TASK NAME | ACTION NAME | EXECUTION TIME | MESSAGE | ACTION STATUS | +--------------------------------------+-------------+-------------+----------------+---------------------------------+--------------------+ | ce2e62ed-826f-4485-a39f-a82bb74338e2 | hello world | hello_world | 0 | Started execution | ACTION_IN_PROGRESS | | ce2e62ed-826f-4485-a39f-a82bb74338e2 | hello world | hello_world | 0 | Finished Execution Successfully | ACTION_SUCCESS | +--------------------------------------+-------------+-------------+----------------+---------------------------------+--------------------+","title":"Start the Worker"},{"location":"setup/local-vagrant/#summary","text":"Getting set up locally is a good way to sample Tinkerbell's functionality. The Vagrant set up is not necessarily intended to be persistent, but while it's up and running, you can use the Provisioner to test out the CLI commands or just explore the stack. If you are looking to extend your local setup to develop or test out other workflows, check out the Extending the Vagrant Setup doc. That's it! Let us know what you think about it on Slack .","title":"Summary"},{"location":"setup/packet-terraform/","text":"Packet Setup with Terraform This setup uses the Packet Terraform provider to create two Packet servers, tf-provisioner and tf-worker , that are attached to the same VLAN. Then uses the hello-world example workflow as an introduction to Tinkerbell. tf-provisioner is will be setup as the Provisioner, running tink-server , boots , nginx to serve osie , hegel and Postgres. tf-worker will be setup as the Worker, able to execute workflows. Prerequisites This guide assumes that you already have: A Packet account . Your Packet API Key and Project ID. The Terraform provider needs to have both to create servers in your account. Make sure the API token is a user API token (created/accessed under API keys in your personal settings). SSH Keys need to be set up on Packet for the machine where you are running Terraform. Terraform uses your ssh-agent to connect to the Provisioner when needed. Double check that the right keys are set. Terraform and the Packet Terraform provider installed on your local machine. Using Terraform The first thing to do is to clone the sandbox repository because it contains the Terraform file required to spin up the environment. git clone https://github.com/tinkerbell/sandbox.git cd sandbox/deploy/terraform The Packet Terraform module requires a couple of inputs, the mandatory ones are the packet_api_token and the project_id . You can define them in a terraform.ftvars file. By default, Terraform will load the file when present. You can create one terraform.tfvars that looks like this: cat terraform.tfvars packet_api_token = \"awegaga4gs4g\" project_id = \"235-23452-245-345\" Otherwise, you can pass the inputs to the terraform command through a file, or in-line with the flag -var \"project_id=235-23452-245-345\" . Once you have your variables set, run the Terraform commands: terraform init --upgrade terraform apply > Apply complete! Resources: 5 added, 0 changed, 1 destroyed. Outputs: provisioner_dns_name = eef33e97.packethost.net provisioner_ip = 136.144.56.237 worker_mac_addr = [ \"1c:34:da:42:d3:20\", ] worker_sos = [ \"4ac95ae2-6423-4cad-b91b-3d8c2fcf38d9@sos.dc13.packet.net\", ] As an output, the terraform apply command returns the IP address of the Provisioner, the MAC address of the Worker, and an address for the SOS console of the Worker which will help you to follow what the Worker is doing. Troubleshooting - Server Creation When creating servers on Packet, you might get an error similar to: > Error: The facility sjc1 has no provisionable c3.small.x86 servers matching your criteria. This error notifies you that the facility you are using (by default sjc1) does not have devices available for c3.small.x86 . You can change your device setting to a different device_type in terraform.tfvars (be sure that layer2 networking is supported for the new device_type ), or you can change facility with the variable facility set to a different one. You can check availability of device type in a particular facility through the Packet CLI using the capacity get command. packet capacity get You are looking for a facility that has a normal level of c3.small.x84 . Troubleshooting - SSH Error > Error: timeout - last error: SSH authentication failed > (root@136.144.56.237:22): ssh: handshake failed: ssh: unable to authenticate, > attempted methods [none publickey], no supported methods remain Terraform uses the Terraform file function to copy the tink directory from your local environment to the Provisioner. You can get this error if your local ssh-agent properly You should start the agent and add the private_key that you use to SSH into the Provisioner. $ ssh-agent $ ssh-add ~/.ssh/id_rsa Then rerun terraform apply . You don't need to run terraform destroy , as Terraform can be reapplied over and over, detecting which parts have already been completed. Troubleshooting - File Error > Error: Upload failed: scp: /root/tink/deploy: Not a directory Sometimes the /root/tink directory is only partially copied onto the the Provisioner. You can SSH onto the Provisioner, remove the partially copied directory, and rerun the Terraform to copy it again. Setting Up the Provisioner SSH into the Provisioner and you will find yourself in a copy of the tink repository: ssh -t root@$(terraform output provisioner_ip) \"cd /root/tink && bash\" You have to define and set Tinkerbell's environment. Use the generate-envrc.sh script to generate the envrc file. Using and setting envrc creates an idempotent workflow and you can use it to configure the setup.sh script. For example changing the OSIE version. ./generate-envrc.sh enp1s0f1 > envrc source ./envrc Then, you run the setup.sh script. ./setup.sh setup.sh uses the envrc to install and configure: tink-server hegel boots postgres nginx to serve OSIE A docker registry. Running Tinkerbell The services in Tinkerbell are containerized, and the daemons will run with docker-compose . You can find the definitions in tink/deploy/docker-compose.yaml . Start all services: cd ./deploy docker-compose up -d To check if all the services are up and running you can use docker-compose as well. The output should look similar to: docker-compose ps > Name Command State Ports ------------------------------------------------------------------------------------------------------------------ deploy_boots_1 /boots -dhcp-addr 0.0.0.0: ... Up deploy_db_1 docker-entrypoint.sh postgres Up 0.0.0.0:5432->5432/tcp deploy_hegel_1 cmd/hegel Up deploy_nginx_1 /docker-entrypoint.sh ngin ... Up 192.168.1.2:80->80/tcp deploy_registry_1 /entrypoint.sh /etc/docker ... Up deploy_tink-cli_1 /bin/sh -c sleep infinity Up deploy_tink-server_1 tink-server Up 0.0.0.0:42113->42113/tcp, 0.0.0.0:42114->42114/tcp You now have a Provisioner up and running on Packet. The next steps take you through creating a workflow and pushing it to the Worker using the hello-world workflow example. If you want to use the example, you need to pull the hello-world image from from Docker Hub to the internal registry. docker pull hello-world docker tag hello-world 192.168.1.1/hello-world docker push 192.168.1.1/hello-world Registering the Worker As part of the terraform apply output you get the MAC address for the worker and it generates a file that contains the JSON describing it. Now time to register it with Tinkerbell. cat /root/tink/deploy/hardware-data-0.json { \"id\": \"0eba0bf8-3772-4b4a-ab9f-6ebe93b90a94\", \"metadata\": { \"facility\": { \"facility_code\": \"ewr1\", \"plan_slug\": \"c2.medium.x86\", \"plan_version_slug\": \"\" }, \"instance\": {}, \"state\": \"\" }, \"network\": { \"interfaces\": [ { \"dhcp\": { \"arch\": \"x86_64\", \"ip\": { \"address\": \"192.168.1.5\", \"gateway\": \"192.168.1.1\", \"netmask\": \"255.255.255.248\" }, \"mac\": \"1c:34:da:5c:36:88\", \"uefi\": false }, \"netboot\": { \"allow_pxe\": true, \"allow_workflow\": true } } ] } } {{% notice note %}} The mac address is the same we get from the Terraform output {{% /notice %}} Now we can push the hardware data to tink-server : docker exec -i deploy_tink-cli_1 tink hardware push < /root/tink/deploy/hardware-data-0.json > 2020/06/17 14:12:45 Hardware data pushed successfully A note on the Worker at this point. Ideally the worker should be kept from booting until the Provisioner is ready to serve it OSIE, but on Packet that probably doesn't happen. Now that the Worker's hardware data is registered with Tinkerbell, you should manually reboot the worker through the Packet CLI , API , or Packet UI. Remember to use the SOS console to check what the Worker is doing. Creating a Template Next, define the template for the workflow. The template sets out tasks for the Worker to preform sequentially. This template contains a single task with a single action, which is to perform \u201chello-world\u201d . Just as in the hello-world example, the hello-world image doesn\u2019t contain any instructions that the Worker will perform. It is just a placeholder in the template so a workflow can be created and pushed to the Worker. cat > hello-world.yml <<EOF version: \"0.1\" name: hello_world_workflow global_timeout: 600 tasks: - name: \"hello world\" worker: \"{{.device_1}}\" actions: - name: \"hello_world\" image: hello-world timeout: 60 EOF Create the template and push it to the tink-server with the tink template create command. $ docker exec -i deploy_tink-cli_1 tink template create --name hello-world < ./hello-world.yml Created Template: 75ab8483-6f42-42a9-a80d-a9f6196130df {{% notice note %}} TIP: export the the template ID as a bash variable for future use. {{% /notice %}} $ export TEMPLATE_ID=75ab8483-6f42-42a9-a80d-a9f6196130df Creating a Workflow The next step is to combine both the hardware data and the template to create a workflow. First, the workflow needs to know which template to execute. The Template ID you should use was returned by tink template create command executed above. Second, the Workflow needs a target, defined by the hardware data. In this example, the target is identified by the MAC address you got back from the terraform apply command Combine these two pieces of information and create the workflow with the tink workflow create command. $ docker exec -i deploy_tink-cli_1 tink workflow create \\ -t $TEMPLATE_ID \\ -r '{\"device_1\":'$(jq .network.interfaces[0].dhcp.mac hardware-data-0.json)'}' Created Workflow: a8984b09-566d-47ba-b6c5-fbe482d8ad7f {{% notice note %}} TIP: export the the workflow ID as a bash variable. {{% /notice %}} $ export WORKFLOW_ID=a8984b09-566d-47ba-b6c5-fbe482d8ad7f The command returns a Workflow ID and if you are watching the logs, you will see: tink-server_1 | {\"level\":\"info\",\"ts\":1592936829.6773047,\"caller\":\"grpc-server/workflow.go:63\",\"msg\":\"done creating a new workflow\",\"service\":\"github.com/tinkerbell/tink\"} Checking Workflow Status You can not SSH directly into the Worker but you can use the SOS or Out of bond console provided by Packet to follow what happens in the Worker during the workflow. You can SSH into the SOS console with: ssh $(terraform output -json worker_sos | jq -r '.[0]') You can also use the CLI from the provisioner to validate if the workflow completed correctly using the tink workflow events command. {{% notice note %}} Note that an event can take ~5 minutes to show up. {{% /notice %}} docker exec -i deploy_tink-cli_1 tink workflow events $WORKFLOW_ID > +--------------------------------------+-------------+-------------+----------------+---------------------------------+--------------------+ | WORKER ID | TASK NAME | ACTION NAME | EXECUTION TIME | MESSAGE | ACTION STATUS | +--------------------------------------+-------------+-------------+----------------+---------------------------------+--------------------+ | ce2e62ed-826f-4485-a39f-a82bb74338e2 | hello world | hello_world | 0 | Started execution | ACTION_IN_PROGRESS | | ce2e62ed-826f-4485-a39f-a82bb74338e2 | hello world | hello_world | 0 | Finished Execution Successfully | ACTION_SUCCESS | +--------------------------------------+-------------+-------------+----------------+---------------------------------+--------------------+ Cleanup You can terminate worker and provisioner with the terraform destroy command: terraform destroy","title":"Packet Setup with Terraform"},{"location":"setup/packet-terraform/#packet-setup-with-terraform","text":"This setup uses the Packet Terraform provider to create two Packet servers, tf-provisioner and tf-worker , that are attached to the same VLAN. Then uses the hello-world example workflow as an introduction to Tinkerbell. tf-provisioner is will be setup as the Provisioner, running tink-server , boots , nginx to serve osie , hegel and Postgres. tf-worker will be setup as the Worker, able to execute workflows.","title":"Packet Setup with Terraform"},{"location":"setup/packet-terraform/#prerequisites","text":"This guide assumes that you already have: A Packet account . Your Packet API Key and Project ID. The Terraform provider needs to have both to create servers in your account. Make sure the API token is a user API token (created/accessed under API keys in your personal settings). SSH Keys need to be set up on Packet for the machine where you are running Terraform. Terraform uses your ssh-agent to connect to the Provisioner when needed. Double check that the right keys are set. Terraform and the Packet Terraform provider installed on your local machine.","title":"Prerequisites"},{"location":"setup/packet-terraform/#using-terraform","text":"The first thing to do is to clone the sandbox repository because it contains the Terraform file required to spin up the environment. git clone https://github.com/tinkerbell/sandbox.git cd sandbox/deploy/terraform The Packet Terraform module requires a couple of inputs, the mandatory ones are the packet_api_token and the project_id . You can define them in a terraform.ftvars file. By default, Terraform will load the file when present. You can create one terraform.tfvars that looks like this: cat terraform.tfvars packet_api_token = \"awegaga4gs4g\" project_id = \"235-23452-245-345\" Otherwise, you can pass the inputs to the terraform command through a file, or in-line with the flag -var \"project_id=235-23452-245-345\" . Once you have your variables set, run the Terraform commands: terraform init --upgrade terraform apply > Apply complete! Resources: 5 added, 0 changed, 1 destroyed. Outputs: provisioner_dns_name = eef33e97.packethost.net provisioner_ip = 136.144.56.237 worker_mac_addr = [ \"1c:34:da:42:d3:20\", ] worker_sos = [ \"4ac95ae2-6423-4cad-b91b-3d8c2fcf38d9@sos.dc13.packet.net\", ] As an output, the terraform apply command returns the IP address of the Provisioner, the MAC address of the Worker, and an address for the SOS console of the Worker which will help you to follow what the Worker is doing.","title":"Using Terraform"},{"location":"setup/packet-terraform/#troubleshooting-server-creation","text":"When creating servers on Packet, you might get an error similar to: > Error: The facility sjc1 has no provisionable c3.small.x86 servers matching your criteria. This error notifies you that the facility you are using (by default sjc1) does not have devices available for c3.small.x86 . You can change your device setting to a different device_type in terraform.tfvars (be sure that layer2 networking is supported for the new device_type ), or you can change facility with the variable facility set to a different one. You can check availability of device type in a particular facility through the Packet CLI using the capacity get command. packet capacity get You are looking for a facility that has a normal level of c3.small.x84 .","title":"Troubleshooting - Server Creation"},{"location":"setup/packet-terraform/#troubleshooting-ssh-error","text":"> Error: timeout - last error: SSH authentication failed > (root@136.144.56.237:22): ssh: handshake failed: ssh: unable to authenticate, > attempted methods [none publickey], no supported methods remain Terraform uses the Terraform file function to copy the tink directory from your local environment to the Provisioner. You can get this error if your local ssh-agent properly You should start the agent and add the private_key that you use to SSH into the Provisioner. $ ssh-agent $ ssh-add ~/.ssh/id_rsa Then rerun terraform apply . You don't need to run terraform destroy , as Terraform can be reapplied over and over, detecting which parts have already been completed.","title":"Troubleshooting - SSH Error"},{"location":"setup/packet-terraform/#troubleshooting-file-error","text":"> Error: Upload failed: scp: /root/tink/deploy: Not a directory Sometimes the /root/tink directory is only partially copied onto the the Provisioner. You can SSH onto the Provisioner, remove the partially copied directory, and rerun the Terraform to copy it again.","title":"Troubleshooting - File Error"},{"location":"setup/packet-terraform/#setting-up-the-provisioner","text":"SSH into the Provisioner and you will find yourself in a copy of the tink repository: ssh -t root@$(terraform output provisioner_ip) \"cd /root/tink && bash\" You have to define and set Tinkerbell's environment. Use the generate-envrc.sh script to generate the envrc file. Using and setting envrc creates an idempotent workflow and you can use it to configure the setup.sh script. For example changing the OSIE version. ./generate-envrc.sh enp1s0f1 > envrc source ./envrc Then, you run the setup.sh script. ./setup.sh setup.sh uses the envrc to install and configure: tink-server hegel boots postgres nginx to serve OSIE A docker registry.","title":"Setting Up the Provisioner"},{"location":"setup/packet-terraform/#running-tinkerbell","text":"The services in Tinkerbell are containerized, and the daemons will run with docker-compose . You can find the definitions in tink/deploy/docker-compose.yaml . Start all services: cd ./deploy docker-compose up -d To check if all the services are up and running you can use docker-compose as well. The output should look similar to: docker-compose ps > Name Command State Ports ------------------------------------------------------------------------------------------------------------------ deploy_boots_1 /boots -dhcp-addr 0.0.0.0: ... Up deploy_db_1 docker-entrypoint.sh postgres Up 0.0.0.0:5432->5432/tcp deploy_hegel_1 cmd/hegel Up deploy_nginx_1 /docker-entrypoint.sh ngin ... Up 192.168.1.2:80->80/tcp deploy_registry_1 /entrypoint.sh /etc/docker ... Up deploy_tink-cli_1 /bin/sh -c sleep infinity Up deploy_tink-server_1 tink-server Up 0.0.0.0:42113->42113/tcp, 0.0.0.0:42114->42114/tcp You now have a Provisioner up and running on Packet. The next steps take you through creating a workflow and pushing it to the Worker using the hello-world workflow example. If you want to use the example, you need to pull the hello-world image from from Docker Hub to the internal registry. docker pull hello-world docker tag hello-world 192.168.1.1/hello-world docker push 192.168.1.1/hello-world","title":"Running Tinkerbell"},{"location":"setup/packet-terraform/#registering-the-worker","text":"As part of the terraform apply output you get the MAC address for the worker and it generates a file that contains the JSON describing it. Now time to register it with Tinkerbell. cat /root/tink/deploy/hardware-data-0.json { \"id\": \"0eba0bf8-3772-4b4a-ab9f-6ebe93b90a94\", \"metadata\": { \"facility\": { \"facility_code\": \"ewr1\", \"plan_slug\": \"c2.medium.x86\", \"plan_version_slug\": \"\" }, \"instance\": {}, \"state\": \"\" }, \"network\": { \"interfaces\": [ { \"dhcp\": { \"arch\": \"x86_64\", \"ip\": { \"address\": \"192.168.1.5\", \"gateway\": \"192.168.1.1\", \"netmask\": \"255.255.255.248\" }, \"mac\": \"1c:34:da:5c:36:88\", \"uefi\": false }, \"netboot\": { \"allow_pxe\": true, \"allow_workflow\": true } } ] } } {{% notice note %}} The mac address is the same we get from the Terraform output {{% /notice %}} Now we can push the hardware data to tink-server : docker exec -i deploy_tink-cli_1 tink hardware push < /root/tink/deploy/hardware-data-0.json > 2020/06/17 14:12:45 Hardware data pushed successfully A note on the Worker at this point. Ideally the worker should be kept from booting until the Provisioner is ready to serve it OSIE, but on Packet that probably doesn't happen. Now that the Worker's hardware data is registered with Tinkerbell, you should manually reboot the worker through the Packet CLI , API , or Packet UI. Remember to use the SOS console to check what the Worker is doing.","title":"Registering the Worker"},{"location":"setup/packet-terraform/#creating-a-template","text":"Next, define the template for the workflow. The template sets out tasks for the Worker to preform sequentially. This template contains a single task with a single action, which is to perform \u201chello-world\u201d . Just as in the hello-world example, the hello-world image doesn\u2019t contain any instructions that the Worker will perform. It is just a placeholder in the template so a workflow can be created and pushed to the Worker. cat > hello-world.yml <<EOF version: \"0.1\" name: hello_world_workflow global_timeout: 600 tasks: - name: \"hello world\" worker: \"{{.device_1}}\" actions: - name: \"hello_world\" image: hello-world timeout: 60 EOF Create the template and push it to the tink-server with the tink template create command. $ docker exec -i deploy_tink-cli_1 tink template create --name hello-world < ./hello-world.yml Created Template: 75ab8483-6f42-42a9-a80d-a9f6196130df {{% notice note %}} TIP: export the the template ID as a bash variable for future use. {{% /notice %}} $ export TEMPLATE_ID=75ab8483-6f42-42a9-a80d-a9f6196130df","title":"Creating a Template"},{"location":"setup/packet-terraform/#creating-a-workflow","text":"The next step is to combine both the hardware data and the template to create a workflow. First, the workflow needs to know which template to execute. The Template ID you should use was returned by tink template create command executed above. Second, the Workflow needs a target, defined by the hardware data. In this example, the target is identified by the MAC address you got back from the terraform apply command Combine these two pieces of information and create the workflow with the tink workflow create command. $ docker exec -i deploy_tink-cli_1 tink workflow create \\ -t $TEMPLATE_ID \\ -r '{\"device_1\":'$(jq .network.interfaces[0].dhcp.mac hardware-data-0.json)'}' Created Workflow: a8984b09-566d-47ba-b6c5-fbe482d8ad7f {{% notice note %}} TIP: export the the workflow ID as a bash variable. {{% /notice %}} $ export WORKFLOW_ID=a8984b09-566d-47ba-b6c5-fbe482d8ad7f The command returns a Workflow ID and if you are watching the logs, you will see: tink-server_1 | {\"level\":\"info\",\"ts\":1592936829.6773047,\"caller\":\"grpc-server/workflow.go:63\",\"msg\":\"done creating a new workflow\",\"service\":\"github.com/tinkerbell/tink\"}","title":"Creating a Workflow"},{"location":"setup/packet-terraform/#checking-workflow-status","text":"You can not SSH directly into the Worker but you can use the SOS or Out of bond console provided by Packet to follow what happens in the Worker during the workflow. You can SSH into the SOS console with: ssh $(terraform output -json worker_sos | jq -r '.[0]') You can also use the CLI from the provisioner to validate if the workflow completed correctly using the tink workflow events command. {{% notice note %}} Note that an event can take ~5 minutes to show up. {{% /notice %}} docker exec -i deploy_tink-cli_1 tink workflow events $WORKFLOW_ID > +--------------------------------------+-------------+-------------+----------------+---------------------------------+--------------------+ | WORKER ID | TASK NAME | ACTION NAME | EXECUTION TIME | MESSAGE | ACTION STATUS | +--------------------------------------+-------------+-------------+----------------+---------------------------------+--------------------+ | ce2e62ed-826f-4485-a39f-a82bb74338e2 | hello world | hello_world | 0 | Started execution | ACTION_IN_PROGRESS | | ce2e62ed-826f-4485-a39f-a82bb74338e2 | hello world | hello_world | 0 | Finished Execution Successfully | ACTION_SUCCESS | +--------------------------------------+-------------+-------------+----------------+---------------------------------+--------------------+","title":"Checking Workflow Status"},{"location":"setup/packet-terraform/#cleanup","text":"You can terminate worker and provisioner with the terraform destroy command: terraform destroy","title":"Cleanup"}]}